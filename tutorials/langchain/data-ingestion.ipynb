{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d2f94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/speech.txt\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fcdc1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'),\n",
       " Document(metadata={}, page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…'),\n",
       " Document(metadata={}, page_content='It will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early'),\n",
       " Document(metadata={}, page_content='and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.'),\n",
       " Document(metadata={}, page_content='We have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the'),\n",
       " Document(metadata={}, page_content='are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except'),\n",
       " Document(metadata={}, page_content='here and there and without countenance except from a lawless and malignant few.'),\n",
       " Document(metadata={}, page_content='It is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy,'),\n",
       " Document(metadata={}, page_content='always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.'),\n",
       " Document(metadata={}, page_content='To such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "text_splitter.create_documents([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3de0479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 100\n",
      "Created a chunk of size 347, which is longer than the specified 100\n",
      "Created a chunk of size 668, which is longer than the specified 100\n",
      "Created a chunk of size 982, which is longer than the specified 100\n",
      "Created a chunk of size 789, which is longer than the specified 100\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20)\n",
    "splitted = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46acb517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Welcome to My Fake Page'}, page_content='Welcome to My Fake Page'),\n",
       " Document(metadata={'Header 1': 'Welcome to My Fake Page'}, page_content=\"This is a completely made-up webpage for demonstration purposes. It's not connected to any real content or services.  \\nA nice placeholder image to fill the space.  \\nHere's some more text.  We can talk about anything, really. This is just to provide some more content to show how paragraphs \\nwork.  \\nYou can visit for more information. (This is a fake link!)  \\nExample Website\"),\n",
       " Document(metadata={'Header 1': 'Welcome to My Fake Page', 'Header 2': 'A Smaller Heading'}, page_content='A Smaller Heading'),\n",
       " Document(metadata={'Header 1': 'Welcome to My Fake Page', 'Header 2': 'A Smaller Heading'}, page_content='Item 1  \\nItem 2  \\nItem 3  \\nFirst Step  \\nSecond Step  \\nThis is preformatted text.\\n    It preserves spaces and line breaks.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\"\n",
    "html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Fake Webpage</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to My Fake Page</h1>\n",
    "    <p>This is a completely made-up webpage for demonstration purposes. It's not connected to any real content or services.</p>\n",
    "    <img src=\"https://via.placeholder.com/300x200\" alt=\"Placeholder Image\">\n",
    "    <p>A nice placeholder image to fill the space.</p>\n",
    "    <p>Here's some more text.  We can talk about anything, really. This is just to provide some more content to show how paragraphs \n",
    "work.</p>\n",
    "    <p>You can visit <a href=\"https://www.example.com\">Example Website</a> for more information. (This is a fake link!) </p>\n",
    "    <h2>A Smaller Heading</h2>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "    </ul>\n",
    "    <ol>\n",
    "    <li>First Step</li>\n",
    "    <li>Second Step</li>\n",
    "    </ol>\n",
    "    <pre>\n",
    "    This is preformatted text.\n",
    "    It preserves spaces and line breaks.\n",
    "    </pre>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_splitter_splits = html_splitter.split_text(html_string)\n",
    "html_splitter_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61248f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='End container NOTE: Script required for drop-down button to work (mirrors).  \\nEnd header wrapper End content End footer  \\nEnd header  \\nEnd navigation End search  \\nStanford Encyclopedia of Philosophy  \\nMenu  \\nBrowse  \\nTable of Contents  \\nWhat\\'s New  \\nRandom Entry  \\nChronological  \\nArchives  \\nAbout  \\nEditorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nContact  \\nSupport SEP  \\nSupport the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries  \\nBegin article sidebar End article sidebar NOTE: Article content must have two wrapper divs: id=\"article\" and id=\"article-content\" End article NOTE: article banner is outside of the id=\"article\" div. End article-banner  \\nEntry Navigation  \\nEntry Contents  \\nBibliography  \\nAcademic Tools  \\nFriends PDF Preview  \\nAuthor and Citation Info  \\nBack to Top  \\nEnd article-content  \\nBEGIN ARTICLE HTML #aueditable DO NOT MODIFY THIS LINE AND BELOW END ARTICLE HTML  \\nDO NOT MODIFY THIS LINE AND ABOVE'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel'}, page_content='Kurt Gödel'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel'}, page_content='First published Tue Feb 13, 2007; substantive revision Fri Dec 11, 2015  \\nKurt Friedrich Gödel (b. 1906, d. 1978) was one of the principal\\nfounders of the modern, metamathematical era in mathematical logic. He\\nis widely known for his Incompleteness Theorems, which are among the\\nhandful of landmark theorems in twentieth century mathematics, but his\\nwork touched every field of mathematical logic, if it was not in most\\ncases their original stimulus. In his philosophical work Gödel\\nformulated and defended mathematical Platonism, the view that\\nmathematics is a descriptive science, or alternatively the view that\\nthe concept of mathematical truth is objective. On the basis of that\\nviewpoint he laid the foundation for the program of conceptual\\nanalysis within set theory (see below). He adhered to Hilbert’s\\n“original rationalistic conception” in mathematics (as he\\ncalled\\n it); and he was prophetic in anticipating and emphasizing the importance\\nof large cardinals in set theory before their importance became\\nclear.  \\n[ ]  \\n1  \\nEntry Contents Entry Contents  \\n1. Biographical Sketch  \\n2. Gödel’s Mathematical Work  \\n2.1 The Completeness Theorem  \\n2.1.1 Introduction  \\n2.1.2 Proof of the Completeness Theorem  \\n2.1.3 An Important Consequence of the Completeness Theorem  \\n2.2 The Incompleteness Theorems  \\n2.2.1 The First Incompleteness Theorem  \\n2.2.2 The proof of the First Incompleteness Theorem  \\n2.2.3 The Second Incompleteness Theorem  \\nSupplementary Document: Did the Incompleteness Theorems Refute Hilbert’s Program?  \\n2.3 Speed-up Theorems  \\n2.4 Gödel’s Work in Set theory  \\n2.4.1 The consistency of the Continuum Hypothesis and the Axiom of Choice  \\n2.4.2 Gödel’s Proof of the Consistency of the Continuum Hypothesis and the Axiom of Choice with the Axioms of Zermelo-Fraenkel Set Theory  \\n2.4.3 Consequences of Consistency  \\n2.4.4 Gödel’s view of the Axiom of Constructibility  \\n2.5 Gödel’s Work in Intuitionistic Logic and Arithmetic  \\n2.5.1 Intuitionistic Propositional Logic is not Finitely-Valued  \\n2.5.2 Classical Arithmetic is Interpretable in Heyting Arithmetic  \\n2.5.3 Intuitionistic Propositional Logic is Interpretable in  \\nS4  \\n2.5.4 Heyting Arithmetic is Interpretable into Computable Functionals of Finite Type.  \\nSupplement Document: Gödel’s Documents  \\n3. Gödel’s Philosophical Views  \\n3.1 Gödel’s Rationalism  \\n3.2 Gödel’s Realism  \\nSupplementary Document: Gödel’s Turn to Phenomenology  \\nSupplementary Document: A Philosophical Argument About the Content of Mathematics  \\nBibliography  \\nPrimary Sources  \\nGödel’s Writings  \\nThe Collected Papers of Kurt Gödel  \\nSelected Works of Kurt Gödel  \\nSecondary Sources  \\nAcademic Tools  \\nOther Internet Resources  \\nRelated Entries  \\n1. Biographical Sketch  \\nKurt Gödel was born on April 28, 1906 in what was then the\\nAustro-Hungarian city of Brünn, and what is now Brno in the Czech\\nRepublic.  \\nGödel’s father Rudolf August was a businessman, and his\\nmother Marianne was a well-educated and cultured woman to whom\\nGödel remained close throughout his life, as witnessed by the\\nlong and wide-ranging correspondence between them. The family was well\\noff, and Gödel’s childhood was an uneventful one, with one\\nimportant exception; namely, from about the age of four Gödel\\nsuffered frequent episodes of poor health, and the health problems he\\nsuffered then as well as others of various kinds were to plague him\\nhis entire life.  \\nHealth problems notwithstanding, Gödel proved to be an exemplary\\nstudent at primary school and later the Gymnasium, excelling\\nespecially in mathematics, languages and religion. Upon his graduation\\nfrom the Gymnasium in Brno in 1924 Gödel enrolled in the\\nUniversity of Vienna, attending lectures on physics, his initial field\\nof interest, lectures on philosophy given by Heinrich Gomperz, and\\nlectures on mathematics. Gödel took a number of physics courses\\nduring his undergraduate years, as witnessed by his university\\ntranscript; this is notable in view of Gödel’s subsequent\\ncontributions to relativity in 1947. Philipp Furtwängler, cousin\\nof the great German conductor Wilhelm Furtwängler, was one of his\\nmathematics professors, and indeed Furtwängler’s course on\\nclass field theory almost tempted Gödel to pursue his studies in\\nthat area. Gödel learned his logic from Rudolph Carnap and from\\nHans Hahn, eventually graduating under Hahn with a Dr.phil. in\\nmathematics in 1929. The main theorem of his dissertation was the\\ncompleteness theorem for first order logic (Gödel\\n 1929).  \\n[ ]  \\n2  \\nGödel’s university years also marked the beginning of his\\nattendance at meetings of the Vienna Circle, a group around Moritz\\nSchlick that quickly became known as “logical\\npositivists,” a term coined by Feigl and Blumberg in their 1931\\n“Logical positivism: A new movement in European\\nphilosophy” (Feigl and Blumberg 1931). Though Gödel was not\\nhimself a logical positivist, those discussions were a crucial\\nformative influence.  \\nThe 1930s were a prodigious decade for Gödel. After publishing\\nhis 1929 dissertation in 1930, he published his groundbreaking\\nincompleteness theorems in 1931, on the basis of which he was granted\\nhis Habilitation in 1932 and a Privatdozentur at the University of\\nVienna in 1933.  \\nAmong his mathematical achievements at the decade’s close is the\\nproof of the consistency of both the Axiom of Choice and\\nCantor’s Continuum Hypothesis with the Zermelo-Fraenkel axioms\\nfor set theory, obtained in 1935 and 1937, respectively. Gödel\\nalso published a number of significant papers on modal and\\nintuitionistic logic and arithmetic during this period, principal\\namong which is his “On intuitionistic arithmetic and number\\ntheory,” (Gödel 1933e), in which he showed that classical\\nfirst order arithmetic is interpretable in Heyting arithmetic by a\\nsimple translation. Other publications of the 1930s include those on\\nthe decision problem for the predicate calculus, on the length of\\nproofs, and on differential and projective geometry.  \\nBy the end of the decade both Gödel’s advisor Hans Hahn and\\nMoritz Schlick had died (the latter was assassinated by an\\nex-student), two events which led to a personal crisis for Gödel.\\nAlso, his appointment at the University, that of Privatdozentur, was\\ncancelled, being replaced by the position “Dozentur neuer\\nOrdnung,” granted to candidates only after they had passed a\\nracial\\n test. Gödel’s three trips the United States during that decade\\ntriggered an investigation. (See Sigmund 2006.) Finally, Gödel\\nwas found fit for military service by the Nazi government in 1939.  \\n[ ]  \\n3  \\nAll of these events were decisive in influencing his decision to leave\\nAustria in 1940, when he and his wife Adele emigrated to the United\\nStates. This long and difficult episode in their life is recounted by\\nJohn Dawson in his biography of Gödel called “Logical\\nDilemmas,” (Dawson 1997) as well as by Solomon Feferman in\\n“Gödel’s Life and Work,” (Feferman 1986) to\\nboth of which the reader is referred.  \\nUpon arrival Gödel took up an appointment as an ordinary member\\nat the Institute for Advanced Study; he would become a permanent\\nmember of the Institute in 1946 and would be granted his professorship\\nin 1953. (Gödel and his wife were granted American citizenship in\\nApril 1948.) He would remain at the Institute until his retirement in\\n1976. The Gödels never returned to Europe.  \\nGödel’s early years at the Institute were notable for his\\nclose friendship with his daily walking partner Albert Einstein, as\\nwell as for his turn to philosophy of mathematics, a field on which\\nGödel began to concentrate almost exclusively from about 1943.\\nThe initial period of his subsequent lifelong involvement with\\nphilosophy was a fruitful one (in terms of publications): in 1944 he\\npublished his first philosophical paper, entitled “On\\nRussell’s Mathematical Logic” (Gödel 1944), and in\\n1947 he published his second, entitled “What is Cantor’s\\nContinuum Hypothesis?” (Gödel 1947). In 1949 he published\\nhis third, entitled “A Remark on the Relationship between\\nRelativity Theory and Idealistic Philosophy.” (Gödel\\n1949a). The latter paper coincided with results on rotating universes\\nin relativity he had obtained in 1949, which were first published in\\nan article entitled: “An Example of a New Type of Cosmological\\nSolutions of Einstein’s Field Equations of Gravitation.”\\n(Gödel 1949).  \\nAmong Gödel’s other significant philosophical works of the\\n1940s must be counted his 1941 lecture entitled “In What Sense\\nis Intuitionistic Logic Constructive?” (Gödel *1941) in\\nwhich the notion: “computable function of finite type” is\\nintroduced. A paper based on the ideas in the lecture entitled\\n“Über eine bisher noch nicht benützte Erweiterung des\\nfiniten Standpunktes,” was published only in 1958, and the\\ninterpretation of Heyting arithmetic into the quantifier free calculus in it became known as the “Dialectica\\nInterpretation,” after the journal in which the article was\\npublished (Gödel 1958). (For the revision of it from 1972, see\\nGödel 1995.) Finally the decade saw the beginning of\\nGödel’s intensive study of Leibniz, which, Gödel\\nreports, occupied the period from 1943 to\\n 1946.  \\nT  \\n[ ]  \\n4  \\nThe 1950s saw a deepening of Gödel’s involvement with\\nphilosophy: In 1951 Gödel delivered a philosophical lecture at\\nBrown University, usually referred to as the Gibbs Lecture, entitled\\n“Some Basic Theorems on the Foundations of Mathematics and Their\\nPhilosophical Implications” (Gödel *1951). From 1953 to\\n1959 Gödel worked on a submission to the Schilpp volume on Rudolf\\nCarnap entitled “Is Mathematics a Syntax of Language?”\\n(Gödel *1953/9-III, Gödel *1953/9-V). Gödel published\\nneither of these two important manuscripts in his lifetime, although\\nboth would appear on two lists which were found in the Gödel\\nNachlass, entitled “Was ich publizieren könnte.” (In\\nEnglish: “What I could publish.” Both manuscripts\\neventually appeared in Gödel 1995.) By the decade’s close\\nGödel developed a serious interest in\\n phenomenology.  \\n[ ]  \\n5  \\nGödel’s final years are notable for his circulation of two\\nmanuscripts: “Some considerations leading to the probable\\nconclusion that the true power of the continuum is\\nℵ ,” (Gödel *1970a, *1970b) his attempt\\nto derive the value of the continuum from the so-called scale axioms\\nof Hausdorff, and his “Ontologischer Beweis,” (Gödel\\n*1970) which he entrusted to Dana Scott in 1970 (though it appears to\\nhave been written earlier). Taken together, the two manuscripts are\\nthe fitting last words of someone who, in a fifty year involvement\\nwith mathematics and philosophy, pursued, or more precisely, for pursuing those two subjects under the\\nsingle heading: “strenge Wissenschaft”—a turn of\\nmind that had been in place from Gödel’s start in 1929,\\nwhen at the age of twenty-three he opened his doctoral thesis with\\nsome philosophical remarks.  \\n2  \\nsought the grounds  \\nGödel died in Princeton on January 14, 1978 at the age of 71. His\\ndeath certificate records the cause of death as “starvation and\\ninanition, due to personality disorder.” His wife Adele survived\\nhim by three years.  \\nFor further biographical material, see Gödel 1987, Kleene 1987,\\nKreisel 1980, Taussky-Todd 1987 and Yourgrau 2005.  \\n2. Gödel’s Mathematical Work  \\nBelow is an examination of some of Gödel’s main\\ncontributions in logic and set theory. This treatment of\\nGödel’s technical work is not exhaustive, omitting\\ndiscussion of Gödel’s work in physics and his work on the\\ndecision problem. These will be treated in the sequel to this\\nentry.  \\nFor a complete chronology of Gödel’s work the reader is\\nreferred to that compiled by John Dawson in volume I of\\nGödel’s Collected Works (Gödel 1986, p. 37).  \\n2.1 The Completeness Theorem  \\n2.1.1 Introduction  \\nThe completeness question for the first order predicate calculus was\\nstated precisely and in print for the first time in 1928 by Hilbert\\nand Ackermann in their text (Hilbert and Ackermann 1928), a text with which Gödel\\nwould have been quite\\n familiar.  \\nGrundzüge der theoretischen\\nLogik  \\n[ ]  \\n6  \\nThe question Hilbert and Ackermann pose is whether a certain\\nexplicitly given axiom system for the first order predicate calculus\\n“…is complete in the sense that from it all logical\\nformulas that are correct for each domain of individuals can be\\nderived…” (van Heijenoort 1967, p. 48).  \\n2.1.2 Proof of the Completeness Theorem  \\nWe give an outline of Gödel’s own proof in his doctoral\\nthesis (Gödel 1929). An essential difference with earlier efforts\\n(discussed below and elsewhere, e.g. in Zach 1999), is that Gödel\\ndefines meticulously all the relevant basic concepts.  \\nA “logical expression” in Gödel’s terminology\\nis a well-formed first order formula without identity. An expression\\nis “refutable” if its negation is provable,\\n“valid” if it is true in every interpretation and\\n“satisfiable” if it is true in some interpretation. The\\nCompleteness Theorem is stated as follows:  \\n. Every valid logical expression is provable. Equivalently, every\\nlogical expression is either satisfiable or refutable.  \\nTheorem 1  \\nGödel’s proof calculus is that of Hilbert and\\nAckermann’s text. An expression is in normal form if all the\\nquantifiers occur at the beginning. The degree of an expression or\\nformula is the number of alternating blocks of quantifiers at the\\nbeginning of the formula, assumed to begin with universal quantifiers.\\nGödel shows that if the completeness theorem holds for formulas\\nof degree it must hold for formulas of degree +\\n1. Thus the question of completeness reduces to formulas of degree 1.\\nThat is, it is to be shown that any normal formula ( )φ\\nof degree 1 is either satisfiable or refutable, where\\n“( )” stands for a (non-empty) block of universal\\nquantifiers followed by a (possibly empty) block of existential\\nones.  \\nk  \\nk  \\nQ  \\nQ  \\nGödel defines a book-keeping device, a well-ordering of all\\ntuples of variables arising from a need to satisfy φ as dictated\\nby ( ). For example, if ( )φ is\\n∀ ∃ ψ( , ), we list the quantifier-free formulas\\nψ( , ). (Or more precisely, finite\\nconjunctions of these in increasing length. See below.) Then in any\\ndomain consisting of the values of the different , in which each\\nψ( , ) is\\ntrue, the sentence ( )φ is clearly true. A crucial lemma\\nclaims the provability, for each , of the formula\\n( )φ →\\n( )φ , where the\\nquantifier free formula φ asserts the truth\\nof ψ for all tuples up to the kth tuple of variables arising from\\n( ), and\\n( )φ is the\\nexistential closure of φ . (See the example\\nbelow where the definition of the φ s\\nis given.) This lemma is the main step missing from the various\\nearlier attempts at the proof due to Löwenheim and Skolem, and,\\nin the context of the completeness theorem for first order logic,\\nrenders the connection between syntax and semantics completely\\nexplicit.  \\nQ  \\nQ  \\nx  \\n0  \\nx  \\n1  \\nx  \\n0  \\nx  \\n1  \\nx  \\nn  \\nx  \\n+1  \\nn  \\nx  \\nn  \\nx  \\nn  \\nx  \\nn+1  \\nQ  \\nk  \\nQ  \\nQ  \\nk  \\nk  \\nk  \\nQ  \\nQ  \\nk  \\nk  \\nk  \\n′  \\nk  \\nLet us consider an example of how a particular formula would be found\\nto be either satisfiable or its negation provable, following\\nGödel’s method: Consider φ =\\n∀ ∃ ψ( , ), where ψ( , ) is quantifier-free. We show that this is\\neither refutable or satisfiable. We make the following\\ndefinitions:  \\nx  \\n0  \\nx  \\n1  \\nx  \\n0  \\nx  \\n1  \\nx  \\n0  \\nx  \\n1  \\nφ is the expression\\nψ( , )  \\n0  \\nx  \\n0  \\nx  \\n1  \\nφ is the expression\\nψ( , ) ∧\\nψ( , )  \\n1  \\nx  \\n0  \\nx  \\n1  \\nx  \\n1  \\nx  \\n2  \\n…  \\nφ is the expression\\nψ( , ) ∧\\n…∧ ψ( , ).  \\nn  \\nx  \\n0  \\nx  \\n1  \\nx  \\nn  \\nx  \\n+1  \\nn  \\nThe crucial lemma, referred to above, shows that from φ we can\\nderive for each ,\\n∃ …∃ φ .  \\nn  \\nx  \\n0  \\nx  \\n+1  \\nn  \\nn  \\nFor some ,\\nφ is not satisfiable. Then, Gödel\\nargued, using the already known completeness theorem for propositional\\n logic, that ¬φ is provable, and hence so is\\n∀ ,…, ¬φ . Thus\\n¬∃ …∃ φ is provable and therefore the ¬φ is provable, i.e., φ is\\nrefutable in the Hilbert-Ackermann system. (Some partial results about\\npropositional logic in addition to those already mentioned include the\\nsemantic completeness of the propositional calculus due to Post\\n(1921), as well as a more general completeness theorem for the same\\ndue to Bernays in 1918; the latter appears in Bernays’\\nunpublished of 1918; see also Bernays\\n1926.)  \\nCase 1:  \\nn  \\nn  \\n[ ]  \\n7  \\nn  \\nx  \\n0  \\nx  \\n+1  \\nn  \\nn  \\nx  \\n0  \\nx  \\n+1  \\nn  \\nn  \\nHabilitationsschrift  \\nEach φ is\\nsatisfiable. There are only finitely many possible models with\\nuniverse { ,…, }.\\nGödel orders them as a tree by defining a model to be\\nbelow a model ′ if is a submodel of ′. In this way we obtain a tree which is finitely\\nbranching but infinite. By König’s Lemma there is an\\ninfinite branch . (In the proof, Gödel explicitly\\nconstructs the branch given by König’s Lemma rather than\\nciting it by name.) The union of the models on forms a\\nmodel with universe { , ,…}. Since satisfies each\\nφ , the original formula φ holds in . So φ is satisfiable and we are done.  \\nCase 2:  \\nn  \\nx  \\n0  \\nx  \\nn+1  \\nM  \\nM  \\nM  \\nM  \\nB  \\nB  \\nM  \\nx  \\n0  \\nx  \\n1  \\nM  \\nn  \\nM  \\nNote that the model, in the satisfiability case of Gödel’s\\nproof, is always countable. Thus this proof of the Completeness\\nTheorem gives also the Löweheim-Skolem Theorem (see below).\\nGödel extends the result to countably many formulas and to the\\ncase of first order logic with identity. He also proves the\\nindependence of the axioms.  \\nIn 1930 Gödel published the paper based on his thesis (Gödel\\n1930) notable also for the inclusion of the compactness theorem, which\\nis only implicitly stated in the thesis. The theorem as stated by\\nGödel in Gödel 1930 is as follows: a countably infinite set\\nof quantificational formulas is satisfiable if and only if every\\nfinite subset of those formulas is satisfiable. Gödel uses\\ncompactness to derive a generalization of the completeness\\ntheorem.  \\nThe Compactness Theorem was extended to the case of uncountable\\nvocabularies by Maltsev in 1936 (see Mal’cev 1971), from which\\nthe Upward Löwenheim-Skolem theorem immediately follows. The\\nCompactness Theorem would become one of the main tools in the then\\nfledgling subject of model theory.  \\n2.1.3 An Important Consequence of the Completeness Theorem  \\nA theory is said to be categorical if it has only one model up to\\nisomorphism; it is λ-categorical if it has only one model of\\ncardinality λ, up to isomorphism. One of the main consequences\\nof the completeness theorem is that categoricity fails for Peano\\narithmetic and for Zermelo-Fraenkel set theory.  \\nIn detail, regarding the first order Peano axioms (henceforth ), the existence of non-standard models of them actually\\nfollows from completeness together with compactness. One constructs\\nthese models, which contain infinitely large integers, as follows: add\\na new constant symbol to the language of arithmetic. Extend to a new theory * by adding to it the infinite\\ncollection of axioms: { > , > , …}, where, e.g., is S(S(S(0))). *\\nis finitely consistent (i.e., every finite subset of * is\\nconsistent) hence consistent, hence by the Completeness Theorem it has\\na model.  \\nPA  \\nc  \\nPA  \\nPA  \\nc  \\n0  \\nc  \\n1  \\n3  \\nPA  \\nPA  \\nThis simple fact about models of Peano arithmetic was not pointed out\\nby Gödel in any of the publications connected with the\\nCompleteness Theorem from that time, and it seems not to have been\\nnoticed by the general logic community until much later.\\nSkolem’s definable ultrapower construction from 1933 (see Skolem\\n1933) gives a direct construction of a non-standard model of True\\nArithmetic (which extends Peano arithmetic, being the set of\\narithmetic sentences true in the natural numbers). But Skolem never\\nmentions the fact that the existence of such models follows from the\\ncompleteness and compactness theorems. Gödel in his review\\n(1934c) of Skolem’s paper also does not mention this fact,\\nrather observing that the failure of categoricity for arithmetic\\nfollows from the theorem.  \\nincompleteness  \\nAs for set theory, the failure of categoricity was already taken note\\nof by Skolem in 1923, because it follows from the\\nLöwenheim-Skolem Theorem (which Skolem arrived at that year; see\\nSkolem 1923, based on Löwenheim 1915 and Skolem 1920): any first\\norder theory in a countable language that has a model has a countable\\nmodel.  \\nSkolem’s observation that categoricity fails for set theory\\nbecause it has countable models is now known as the Skolem\\n paradox. The\\n observation is strongly emphasized in Skolem’s paper, which is\\naccordingly entitled ‘An Observation on the Axiomatic\\nFoundations of Set Theory’ As he wrote in the conclusion of it,\\nhe had not pointed out the relativity in set theory already in 1915\\nbecause:  \\n[ ]  \\n8  \\n… first, I have in the meantime been occupied with other\\nproblems; second, I believed that it was so clear that axiomatization\\nin terms of sets was not a satisfactory ultimate foundation of\\nmathematics that mathematicians would, for the most part, not be very\\nmuch concerned with it. But in recent times I have seen to my surprise\\nthat so many mathematicians think that these axioms of set theory\\nprovide the ideal foundation for mathematics; therefore it seemed to\\nme that the time had come to publish a critique. (English translation\\ntaken from van Heijenoort 1967, p. 300.)  \\nAs an aside, in the proof of the Löwenheim-Skolem theorem,\\nspecifically that part of the theorem in which one constructs a model\\nfor a satisfiable sentence, Löwenheim and Skolem’s tree\\nconstruction was more or less the same as appears in\\nGödel’s thesis. In a 1967 letter to Hao Wang, Gödel\\ntakes note of the fact that his completeness proof had almost been\\nobtained by Skolem in 1923. Though van Heijenoort and Dreben (Dreben\\nand van Heijenoort 1986) remark that “Throughout much of the\\n1920s it was not semantic completeness but the decision problem for\\nquantificational validity, a problem originating from the work of\\nSchröder and Löwenheim, that was the dominant concern in\\nstudying quantification theory” (examples of such results would\\ninclude the decision procedure for the first order monadic predicate\\ncalculus due to Behmann, (Behmann 1922)), according to Gödel, the\\nreasons that Skolem did not obtain the complete proof are different\\nand philosophically important, having to do with the then dominant\\nbias against semantics and against infinitary methods:  \\nThe Completeness Theorem, mathematically, is indeed an almost trivial\\nconsequence of Skolem 1923. However, the fact is that, at that time,\\nnobody (including Skolem himself) drew this conclusion neither from\\nSkolem 1923 nor, as I did, from similar considerations of his own\\n…This blindness (or prejudice, or whatever you may call it) of\\nlogicians is indeed surprising. But I think the explanation is not\\nhard to find. It lies in the widespread lack, at that time, of the\\nrequired epistemological attitude toward metamathematics and toward\\nnon-finitary reasoning. (Gödel 2003b).  \\nThe matter of Skolem’s contribution to the Completeness Theorem\\nhas been extensively discussed in van Atten and Kennedy 2009, as well\\nas in van Atten 2005.  \\n2.2 The Incompleteness Theorems  \\nGödel mentioned the possibility of the unsolvability of a\\nquestion about the reals already in his 1929 thesis, in arguing\\nagainst the formalist principle of Hilbert’s, that consistency\\nis a criterion for existence. In fact, giving a finitary proof of the\\nconsistency of analysis was a key desideratum of what was then known\\nas the Hilbert program, along with proving its completeness.\\nAccordingly it was Gödel’s turn to these questions,\\nespecially the first, which led him to the two incompleteness\\ntheorems. (For a discussion of the Hilbert Program the reader is\\nreferred to the standard references: Sieg 1990, 1988, 1999; Mancosu\\n1998, Zach 2003, Tait 1981 and Tait 2002.)  \\nThe First Incompleteness Theorem provides a counterexample to\\ncompleteness by exhibiting an arithmetic statement which is neither\\nprovable nor refutable in Peano arithmetic, though true in the\\nstandard model. The Second Incompleteness Theorem shows that the\\nconsistency of arithmetic cannot be proved in arithmetic itself. Thus\\nGödel’s theorems demonstrated the infeasibility of the\\nHilbert program, if it is to be characterized by those particular\\ndesiderata, consistency and completeness.  \\nAs an aside, von Neumann understood the two theorems this way, even\\nbefore Gödel did. In fact von Neumann went much further in taking\\nthe view that they showed the infeasibility of classical mathematics\\naltogether. As he wrote to Carnap in June of 1931:  \\nThus today I am of the opinion that 1. Gödel has shown the\\nunrealizability of Hilbert’s program. 2. There is no more reason\\nto reject intuitionism (if one disregards the aesthetic issue, which\\nin practice will also for me be the decisive factor). Therefore I\\nconsider the state of the foundational discussion in Königsberg\\nto be outdated, for Gödel’s fundamental discoveries have\\nbrought the question to a completely different\\n level.  \\n[ ]  \\n9  \\nAnd the previous fall von Neumann had written to Gödel in even\\nstronger terms:  \\nThus, I think that your result has solved negatively the foundational\\nquestion: there is no rigorous justification for classical\\nmathematics. (Gödel 2003b, p. 339)  \\nIt would take Gödel himself a few years to see that those aspects\\nof the Hilbert Program had been decisively refuted by his results\\n(Mancosu 2004).  \\n2.2.1 The First Incompleteness Theorem  \\nIn his (Wang 1996) Hao Wang published the\\nfull text of material Gödel had written (at Wang’s request)\\nabout his discovery of the incompleteness theorems. This material had\\nformed the basis of Wang’s “Some Facts about Kurt\\nGödel,” and was read and approved by Gödel:  \\nLogical Journey  \\nIn the summer of 1930 I began to study the consistency problem of\\nclassical analysis. It is mysterious why Hilbert wanted to prove\\ndirectly the consistency of analysis by finitary methods. I saw two\\ndistinguishable problems: to prove the consistency of number theory by\\nfinitary number theory and to prove the consistency of analysis by\\nnumber theory … Since the domain of finitary number theory was\\nnot well-defined, I began by tackling the second half… I\\nrepresented real numbers by predicates in number theory… and\\nfound that I had to use the concept of truth (for number theory) to\\nverify the axioms of analysis. By an enumeration of symbols, sentences\\nand proofs within the given system, I quickly discovered that the\\nconcept of arithmetic truth cannot be defined in arithmetic. If it\\nwere possible to define truth in the system itself, we would have\\nsomething like the liar paradox, showing the system to be\\ninconsistent… Note that this argument can be formalized to show\\nthe existence of undecidable propositions without giving any\\nindividual instances. (If there were no undecidable propositions, all\\n(and only) true propositions would be provable within the system. But\\nthen we would have a contradiction.)… In contrast to truth,\\nprovability in a given formal system is an explicit combinatorial\\nproperty of certain sentences of the system, which is formally\\nspecifiable by suitable elementary means…  \\nWe see that Gödel first tried to reduce the consistency problem\\nfor analysis to that of arithmetic. This seemed to require a truth\\ndefinition for arithmetic, which in turn led to paradoxes, such as the\\nLiar paradox (“This sentence is false”) and Berry’s\\nparadox (“The least number not defined by an expression\\nconsisting of just fourteen English words”). Gödel then\\nnoticed that such paradoxes would not necessarily arise if truth were\\nreplaced by provability. But this means that arithmetic truth and\\narithmetic provability are not co-extensive — whence the First\\nIncompleteness Theorem.  \\nThis account of Gödel’s discovery was told to Hao Wang very\\nmuch after the fact; but in Gödel’s contemporary\\ncorrespondence with Bernays and Zermelo, essentially the same\\ndescription of his path to the theorems is given. (See Gödel\\n2003a and Gödel 2003b respectively.) From those accounts we see\\nthat the undefinability of truth in arithmetic, a result credited to\\nTarski, was likely obtained in some form by Gödel by 1931. But he\\nneither publicized nor published the result; the biases logicians had\\nexpressed at the time concerning the notion of truth, biases which\\ncame vehemently to the fore when Tarski announced his results on the\\nundefinability of truth in formal systems 1935, may have served as a\\ndeterrent to Gödel’s publication of that theorem.  \\n2.2.2 The proof of the First Incompleteness Theorem  \\nWe now describe the proof of the two theorems, formulating\\nGödel’s results in Peano arithmetic. Gödel himself\\nused a system related to that defined in Principia Mathematica, but\\ncontaining Peano arithmetic. In our presentation of the First and\\nSecond Incompleteness Theorems we refer to Peano arithmetic as , following Gödel’s notation.  \\nP  \\nBefore proceeding to the details of the formal proof, we define the\\nnotion of ω-consistency used by Gödel in the First\\nIncompleteness Theorem: is if ⊢ ¬φ( ) for all implies ⊬ ∃ φ( ).\\nNaturally this implies consistency and follows from the assumption\\nthat the natural numbers satisfy the axioms of Peano arithmetic.  \\nP  \\nω-consistent  \\nP  \\nn  \\nn  \\nP  \\nx  \\nx  \\nOne of the main technical tools used in the proof is , a mechanism which assigns natural numbers to terms and\\nformulas of our formal theory . There are different ways of\\ndoing this. The most common is based on the unique representation of\\nnatural numbers as products of powers of primes. Each symbol of number theory is assigned a positive natural number\\n#( ) in a fixed but arbitrary way, e.g.  \\nGödel\\nnumbering  \\nP  \\ns  \\ns  \\n#(0) = 1  \\n#(=) = 5  \\n#(¬) = 9  \\n#(1) = 2  \\n#(\\u2009(\\u2009) = 6  \\n#(∀) = 10  \\n#(+) = 3  \\n#(\\u2009)\\u2009) = 7  \\n#( ) = 11 +  \\nv  \\ni  \\ni  \\n#(×) = 4  \\n#(∧) = 8  \\nThe natural number corresponding to a sequence = < ,…, >\\nof symbols is  \\nw  \\nw  \\n0  \\nw  \\nk  \\n=\\n2 ·\\n3 · … · ,  \\n⌈  \\nw  \\n⌉  \\n#( )  \\nw  \\n0  \\n#( )  \\nw  \\n1  \\np  \\nk  \\n#( )  \\nw  \\nk  \\nwhere is the +1st prime. It\\nis called its Gödel number and denoted by . In this way we can\\nassign Gödel numbers to formulas, sequences of formulas (once a\\nmethod for distinguishing when one formula ends and another begins has\\nbeen adopted), and most notably, proofs.  \\np  \\nk  \\nk  \\n⌈  \\nw  \\n⌉  \\nAn essential point here is that when a formula is construed as a\\nnatural number, then the numeral corresponding to that natural number\\ncan occur as the argument of a formula, thus enabling the syntax to\\n“refer” to itself, so to speak (i.e., when a numeral is\\nsubstituted into a formula the Gödel number of which the numeral\\nrepresents). This will eventually allow Gödel to formalize the\\nLiar paradox (with “provability” in place of\\n“truth”) by substituting into the formula which says,\\n‘the formula, whose code is , is unprovable,’\\nits own natural number code (or more precisely the corresponding\\nnumeral).  \\nx  \\nAnother concept required to carry out the formalization is the concept\\nof numeralwise expressibility of number theoretic predicates. A\\nnumber-theoretic formula φ( , …, ) is in if for each tuple of natural numbers\\n( , …, ):  \\nn  \\n1  \\nn  \\nk  \\nnumeralwise expressible  \\nP  \\nn  \\n1  \\nn  \\nk  \\n⊨\\nφ( , …, )  \\nN  \\nn  \\n1  \\nn  \\nk  \\n⇒  \\n⊢\\nφ( , …, )  \\nP  \\nn  \\n1  \\nn  \\nk  \\n⊨\\n¬φ( , …, )  \\nN  \\nn  \\n1  \\nn  \\nk  \\n⇒  \\n⊢\\n¬φ( , …, )  \\nP  \\nn  \\n1  \\nn  \\nk  \\nwhere is the formal term which denotes the natural\\nnumber . (In , this is ( (… (0)…), where is the number of iterations of the successor function applied to the\\nconstant symbol 0.) One of the principal goals is to numeralwise\\nexpress the predicate  \\nn  \\nn  \\nP  \\nS  \\nS  \\nS  \\nn  \\nPrf( , ): ‘the sequence with Gödel\\nnumber is a proof of the sentence with Gödel number .’  \\nx  \\ny  \\nx  \\ny  \\nReaching this goal involves defining forty-five relations, each\\ndefined in terms of the preceding ones. These relations are all\\nprimitive\\n recursive. Relations needed are, among others, those which assert of a natural\\nnumber that it codes a sequence, or a formula, or an axiom, or that it\\nis the code, denoted by\\nSb( ),\\nof a formula obtained from a formula with code by\\nsubstituting for its free variable the th numeral for = 1,\\n…, . The forty-fifth primitive recursive relation\\ndefined is Prf( , ), and the forty-sixth is  \\n[ ]  \\n10  \\nr  \\n…  \\nu  \\n1  \\nu  \\nn  \\n( )… ( )  \\nZ  \\nx  \\n1  \\nZ  \\nx  \\nn  \\nr  \\nu  \\ni  \\nx  \\ni  \\ni  \\nn  \\nx  \\ny  \\nProv( ): ‘the sentence with Gödel number is provable in ’  \\ny  \\ny  \\nP  \\nwhich without being primitive recursive, is however obtained from\\nPrf( , ) by existentially quantifying .\\n(Prov( ) satisfies only the ‘positive’ part of\\nnumeralwise expressibility, and not the negative part; but the\\nnegative part is not needed.)  \\nx  \\ny  \\nx  \\ny  \\nIn Theorem V of his paper, Gödel proves that any number theoretic\\npredicate which is primitive recursive is numeralwise expressible in . Thus since Prf( , ) and substitution\\nare primitive recursive, these are decided by when closed\\nterms are substituted for the free variables and . This is the heart of the matter as we will see. Another\\nkey point about numeralwise expressibility is that although we\\ninformally interpret, for example,\\nProv(Sb( )),\\nby: ‘the formula with Gödel number is provable\\nif the Gödel number for the th numeral is\\nsubstituted in place of the th variable,’ neither the\\nformal statement within the theory nor anything we prove\\nabout it appeals to such meanings. On the contrary\\nProv(Sb( )),\\nis a meaningless string of logical and arithmetical symbols. As\\nGödel puts it in his introduction to his theorem V, ‘The\\nfact that can be formulated vaguely by saying that every recursive\\nrelation is definable in the system (if the usual meaning\\nis given to the formulas of this system) is expressed in precise\\nlanguage, reference to any interpretation of the\\nformulas of , by the following Theorem (V) (Gödel 1986,\\np. 171, italics Gödel’s).  \\nP  \\nx  \\ny  \\nP  \\nx  \\ny  \\nr  \\n…  \\nu  \\n1  \\nu  \\nn  \\n( )… ( )  \\nZ  \\nx  \\n1  \\nZ  \\nx  \\nn  \\nr  \\nx  \\ni  \\ni  \\nP  \\nr  \\n…  \\nu  \\n1  \\nu  \\nn  \\n( )… ( )  \\nZ  \\nx  \\n1  \\nZ  \\nx  \\nn  \\nP  \\nwithout  \\nP  \\nGödel in his incompleteness theorems uses a method given in what\\nis called nowadays Gödel’s Fixed Point Theorem. Although\\nGödel constructs a fixed point in the course of proving the\\nincompleteness theorem, he does not state the fixed point theorem\\nexplicitly. The fixed point theorem is as follows:  \\n(Gödel’s Fixed Point Theorem) If φ( ) is a formula of number theory, then\\nthere is a sentence ψ such that ⊢ ψ ↔\\nφ( ), where is the formal term\\ncorresponding to the natural number code of ψ .  \\nTheorem 2  \\nv  \\n0  \\nP  \\n⌈  \\nψ  \\n⌉  \\n⌈  \\nψ  \\n⌉  \\n⌈  \\n⌉  \\nLet σ( , , ) be a\\nformula that numeralwise expresses the number theoretic predicate\\n‘ is the Gödel number of the formula obtained by\\nreplacing the variable in the formula whose\\nGödel number is by the term ’.\\nLet θ( ) be the formula\\n∃ (φ( ) ∧\\nσ( , , )). Let = θ( ) and ψ = θ( ). Now directly by the construction ⊢ ψ ↔\\nφ( ψ ).  \\nProof:  \\nx  \\ny  \\nz  \\ny  \\nv  \\n0  \\nx  \\nz  \\nv  \\n0  \\nv  \\n1  \\nv  \\n1  \\nv  \\n0  \\nv  \\n1  \\nv  \\n0  \\nk  \\n⌈  \\nv  \\n0  \\n⌉  \\nk  \\nP  \\n⌈  \\n⌉  \\nA sentence is refutable from a theory if its negation is provable. The\\nFirst Incompleteness Theorem as Gödel stated it is as\\nfollows:  \\n(Gödel’s First Incompleteness\\nTheorem) If is ω-consistent, then there is a sentence which is\\nneither provable nor refutable from .  \\nTheorem 3  \\nP  \\nP  \\nBy judicious coding of syntax referred to above, write\\na formula\\n Prf( , ) of number theory, representable in , so that  \\nProof:  \\nx  \\ny  \\n[ ]  \\n11  \\nP  \\ncodes a proof of φ ⇒ ⊢\\nPrf( , ).  \\nn  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nand  \\ndoes not code a proof of φ ⇒ ⊢ ¬Prf( , ).  \\nn  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nLet Prov( ) denote the formula ∃ Prf( , ) .\\n By Theorem 2 there is a sentence φ with the property  \\ny  \\nx  \\nx  \\ny  \\n[ ]  \\n12  \\n⊢ (φ ↔\\n¬Prov( )).  \\nP  \\n⌈  \\nφ  \\n⌉  \\nThus φ says ‘I am not provable.’ We now observe, if ⊢ φ, then by (1) there is such that ⊢ Prf( , ), hence ⊢ Prov( ), hence,\\nby (3) ⊢ ¬φ, so is inconsistent.\\nThus  \\nP  \\nn  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nP  \\n⌈  \\nφ  \\n⌉  \\nP  \\nP  \\n⊬ φ  \\nP  \\nFurthermore, by (4) and (2), we have ⊢\\n¬Prf( , ) for all natural\\nnumbers . By ω-consistency ⊬\\n∃ Prf( , ). Thus (3) gives ⊬ ¬φ. We have shown that if is\\nω-consistent, then φ is independent of .  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nn  \\nP  \\nx  \\nx  \\n⌈  \\nφ  \\n⌉  \\nP  \\nP  \\nP  \\nOn concluding the proof of the first theorem, Gödel remarks,\\n“we can readily see that the proof just given is constructive;\\nthat is … proved in an intuitionistically unobjectionable\\nmanner…” (Gödel 1986, p. 177). This is because, as\\nhe points out, all the existential statements are based on his theorem\\nV (giving the numeralwise expressibility of primitive recursive\\nrelations), which is intuitionistically unobjectionable.  \\n2.2.3 The Second Incompleteness Theorem  \\nThe Second Incompleteness Theorem establishes the unprovability, in\\nnumber theory, of the consistency of number theory. First we have to\\nwrite down a number-theoretic formula that expresses the consistency\\nof the axioms. This is surprisingly simple. We just let\\nCon( ) be the sentence ¬Prov( ).  \\nP  \\n⌈  \\n0 =\\n1  \\n⌉  \\n(Gödel’s Second Incompleteness\\nTheorem) If is consistent, then Con( ) is not\\nprovable from .  \\nTheorem 4  \\nP  \\nP  \\nP  \\nLet φ be as in (3). The reasoning used to infer\\n‘if ⊢ φ, then ⊢ 0 ≠\\n1‘ does not go beyond elementary number theory, and can\\ntherefore, albeit with a lot of effort (see below), be formalized in . This yields: ⊢\\n(Prov( ) →\\n¬Con( )), and thus by (3), ⊢\\n(Con( ) → φ). Since ⊬ φ, we\\nmust have ⊬ Con( ).  \\nProof:  \\nP  \\nP  \\nP  \\nP  \\n⌈  \\nφ  \\n⌉  \\nP  \\nP  \\nP  \\nP  \\nP  \\nP  \\nThe above proof (sketch) of the Second Incompleteness Theorem is\\ndeceptively simple as it avoids the formalization. A rigorous proof\\nwould have to establish the proof of ‘if ⊢\\nφ, then ⊢ 0 ≠ 1’ in .  \\nP  \\nP  \\nP  \\nIt is noteworthy that ω-consistency is not needed in the proof\\nof Gödel’s Second Incompleteness Theorem. Also note that\\nneither is ¬Con( ) provable, by the consistency of and the fact, now known as Löb’s theorem, that ⊢\\nProv( ) implies ⊢ φ.  \\nP  \\nP  \\nP  \\n⌈  \\nφ  \\n⌉  \\nP  \\nThe assumption of ω-consistency in the First Incompleteness\\nTheorem was eliminated by Rosser in 1936, and replaced by the weaker\\nnotion of consistency. Rosser’s generalization involves applying\\nthe fixed point theorem to the formula ( ):\\n‘for all : either is not the Gödel\\nnumber of a proof of the formula with Gödel number or\\nthere is a proof shorter than of the negation of (the\\nformula with Gödel number) ’ (see Rosser\\n1936).  \\nR  \\nx  \\nz  \\nz  \\nx  \\nz  \\nx  \\nWith regard to the Second Incompleteness Theorem, the argument relies\\nin part on formalizing the proof of the First Incompleteness Theorem\\nas we saw. This step is omitted in Gödel 1931. He planned to\\ninclude the step in what would have been a second part II (see\\nfootnote 48a of Gödel 1931). But instead of writing it he turned\\nto the continuum\\n problem. (Part II was to elaborate on other points too: the ‘true reason\\nfor incompleteness,’ and the applicability of the two theorems\\nto other systems.) He perhaps did not feel compelled to attend to what\\nlooked like an exercise in formalization, relying instead on the\\ninformal argument to convince (in which it succeeded). However this\\nstep turned out to be somewhat non-trivial. As Kleene puts it in his\\nintroduction to Gödel 1931, of the informal presentation,\\n“Certainly the idea of the argument for Theorem XI (consistency)\\nwas very convincing; but it turned out that the execution of the\\ndetails required somewhat more work and care than had been\\nanticipated.” (See pp. 126–141 of Gödel 1986.)\\nEventually a complete proof of the Second Theorem was given by Hilbert\\nand Bernays in some seventy pages in their Hilbert and Bernays 1939. A\\nmuch more compact treatment of the theorem was given by Löb in\\nhis Löb 1956, and subsequently Feferman, in his 1960\\n“Arithmetization of Metamathematics in a General Setting”\\n(Feferman 1960/1961), gave a succinct and completely general treatment\\nof both the First and Second Theorems. But see the supplementary\\ndocument:  \\n[ ]  \\n13  \\nDid the Incompleteness Theorems Refute Hilbert’s Program?  \\nFor more detailed discussion, see the entry on .  \\nGödel’s incompleteness theorems  \\n2.3 Speed-up Theorems  \\nGödel’s 1936 ‘Speed-up’ theorem, published in\\nan abstract “On the length of proofs”, Gödel 1936\\nsays that while some sentences of arithmetic are true but unprovable,\\nthere are other sentences which are provable, but even the shortest\\nproof is longer than any bound given in advance as a recursive\\nfunction of the sentence. More exactly:  \\n. Given any recursive function there are provable sentences\\nφ of arithmetic such that the shortest proof is greater than ( φ ) in length.  \\nTheorem 5  \\nf  \\nf  \\n⌈  \\n⌉  \\nThe proof we will outline is sensitive to the particular concept we\\nuse for the length of a proof. Another possibility, and the one that\\nGödel has in mind, is the number of formulas in the proof. Buss\\n(see below) proves the theorem in either case, so both cases are\\nresolved.  \\nLet be total recursive function. By\\nGödel’s Fixed Point theorem there is a formula\\nφ( ) stating ‘φ( ) has no proof in PA\\nshorter than ( )’. This is tenable if the\\nlength is measured by number of symbols, because we only need to\\nsearch through finitely many proofs shorter than ( ). Note that φ( ) is true for all , for if φ( ) were false, then there would be a\\nshort proof of φ( ), and hence by soundness\\nφ( ) would be true, a contradiction: φ( )\\nwould both true and false. This can be formalized in PA and thus we\\nget the result that for each the sentence φ( )\\nis provable in PA. Since φ( ) is true for all ,\\nit cannot have a proof in PA which is shorter than ( ).  \\nProof:  \\nf  \\nn  \\nn  \\nf  \\nn  \\nf  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nf  \\nn  \\nThe Speed-up Theorem is the result of contemplating and elaborating\\nthe proof of the incompleteness theorem. It applies the fixed-point\\ntechnique to the concept of unprovability by a short proof, as opposed\\nto the original idea of applying the fixed-point theorem to mere\\nunprovability. The proof has very much the same flavor as the proof of\\nthe incompleteness theorem. Interestingly, it dates from the same year\\nas the construction, due to Rosser, that eliminates the use of\\nω-consistency in the first Incompleteness Theorem; like the\\nSpeed-up Theorem of Gödel, Rosser’s construction exploits\\nthe issue of short and long proofs. Gödel never submitted a proof\\nfor the Speed-up Theorem. Over the years several related proofs were\\npublished, but the first full proof of Gödel’s original\\nresult was given only in 1994 by Sam Buss in his ‘On\\nGödel’s theorems on lengths of proofs I: Number of lines\\nand speedups for arithmetic.’ (Buss 1994). Buss also gives a\\nsecond proof of the theorem which avoids self-reference, following a\\ntechnique due to Statman. Gödel measures the length of proofs by\\nthe number of formulas; but there are also other possibilities, such\\nas the number of symbols in the proof. The case of the Speed-up\\nTheorem where the length of proof is measured by the number of symbols\\nwas proved by Mostowski in 1952 (Mostowski 1982). For proofs of\\nsimilar results see Ehrenfeucht and Mycieleski 1971, and Parikh 1971.\\nThough both measures may be equally natural candidates for measuring\\nthe length of a proof, proving the theorem for length measured by the\\nnumber of symbols avoids a technical complication introduced by the\\nother measure: there are only finitely many proofs with a given number\\nof symbols, whereas there are infinitely many proofs with a given\\nnumber of formulas.  \\nGödel states the Speed-up Theorem differently from the above. Let be the system of logic of the -th\\norder, the variables of the first level being thought of as ranging\\nover natural numbers. In this setting, variables of the second level\\nrange over sets of natural numbers and so on. Gödel’s\\nformulation is:  \\nS  \\nn  \\nn  \\n. Let be a natural number > 0. If is a\\ncomputable function, then there are infinitely many formulas , provable in , such that if is the length of the shortest proof of in and is the length of the\\nshortest proof of in ,\\nthen > ( ).  \\nTheorem 6  \\nn  \\nf  \\nA  \\nS  \\nn  \\nk  \\nA  \\nS  \\nn  \\nl  \\nA  \\nS  \\n+1  \\nn  \\nk  \\nf  \\nl  \\nThe idea is the following: Let\\nφ( ) be a formula, like above, for which\\nφ( ) does not have a short proof in for any . Suppose we have a\\nhigher type system in which we can\\nprove ∀ φ( ). This proof is of constant\\nlength. Thus each φ( ) is derivable from this universal\\nstatement by one application of the logical rule\\n∀ φ( ) → φ( ). Thus\\nφ( ) has in that system for all a short\\nproof.  \\nProof sketch:  \\nx  \\nm  \\nS  \\nn  \\nm  \\nS  \\n+1  \\nn  \\nx  \\nx  \\nm  \\nx  \\nx  \\nt  \\nm  \\nm  \\nWhat kind of stronger system can we have in which\\n∀ φ( ) is provable? We may consider\\nsecond order logic in which we can define a predicate ( ) for the set of natural numbers and furthermore\\ncan prove of a new predicate symbol ( ) that it\\nsatisfies the inductive clauses of the truth definition of first order\\nformulas of arithmetic, relativized to . Then the stronger\\nsystem can prove that provable first order sentences of arithmetic\\nsatisfy the predicate . By the above argument, we can\\nprove in the stronger system that ∀ φ( )\\nsatisfies . Then by adding a few lines we can prove each\\nφ( ) satisfies . Because of the nature of\\nφ( ), this implies the stronger system has a (short)\\nproof of φ( ). An alternative system is Peano’s\\naxioms PA in an extended language where we have a new predicate symbol and axioms stating that the predicate codes\\nthe satisfaction relation for all sentences of the vocabulary not\\ncontaining .  \\nx  \\nx  \\nN  \\nx  \\nTr  \\nx  \\nN  \\nTr  \\nx  \\nx  \\nTr  \\nn  \\nTr  \\nn  \\nn  \\nTr  \\nTr  \\nTr  \\n2.4 Gödel’s Work in Set theory  \\n2.4.1 The consistency of the Continuum Hypothesis and the Axiom of Choice  \\nGödel’s proof of the consistency of the continuum\\nhypothesis with the axioms of Zermelo-Fraenkel set theory is a tour de\\nforce and arguably the greatest achievement of his mathematical life.\\nThis is because aside from the arithmetization, virtually all of the\\ntechnical machinery used in the proof had to be invented ab\\ninitio.  \\nThe Continuum Hypothesis (henceforth ) was formulated by\\nGeorg Cantor, and was the first problem on Hilbert’s list of\\ntwenty-three unsolved problems as given in his famous address to the\\nInternational Mathematical Congress in Paris in 1900. The problem as\\nstated by Hilbert is as follows: Let be an infinite set of\\nreal numbers. Then is either countable, or has cardinality\\n2 , i.e., is in one-to-one\\ncorrespondence either with the set of natural numbers or with the set\\nof all real numbers (otherwise known as the continuum). Another way to\\nstate the continuum hypothesis is that (the first uncountably infinite\\ncardinal) ℵ =\\n2 .  \\nCH  \\nA  \\nA  \\nℵ  \\n0  \\nA  \\n1  \\nℵ  \\n0  \\nAs early as 1922 Skolem speculated that the was\\nindependent of the axioms for set theory given by Zermelo in 1908.\\nNevertheless Hilbert published a (false) proof of the in\\nHilbert 1926. In 1937 Gödel proved its consistency with the\\naxioms of set theory. (Henceforth we use the standard\\nabbreviations for Zermelo-Fraenkel set theory, , and\\nZermelo-Fraenkel set theory with the Axiom of Choice, .)\\nThe consistency of the negation of the was shown by Paul\\nCohen in 1961 (see Cohen 1963) and hence together with\\nGödel’s result one infers that the is\\nindependent of (and ).  \\nCH  \\nCH  \\nZF  \\nZF  \\nZFC  \\nCH  \\nCH  \\nZF  \\nZFC  \\nCohen invented an important new technique called forcing in the course\\nof proving his result; this technique is at present the main method\\nused to construct models of set theory. Forcing led to a revival of\\nformalism among set theorists, the plurality of models being an\\nindication of the “essential variability in set theory,”\\n(Dehornoy 2004) and away from the notion that there is an intended\\nmodel of set theory—a perspective Gödel advocated since at\\nleast 1947, if not\\n earlier. Recently there have been signs that the may again be\\ncoming to be regarded as a problem to be solved mathematically (with\\nthe help of course of some new evident axioms extending ZF). (See for\\nexample Woodin 2001a, 2002, 2001b, and Foreman 1998.) If any of the\\nproposed solutions gain acceptance, this would confirm\\nGödel’s view that the would eventually be\\ndecided by finding an evident extension of the ZF axioms for set\\ntheory. The program associated with this view is called\\n“Gödel’s Large Cardinal Program.”  \\n[ ]  \\n14  \\nCH  \\nCH  \\n2.4.2 Gödel’s Proof of the Consistency of the Continuum Hypothesis and the Axiom of Choice with the Axioms of Zermelo-Fraenkel Set Theory  \\nThe continuum problem is shown to be consistent with ZF by finding an\\nenumeration of the reals which is indexed by the countable ordinals, a\\nstrategy which had been recognized as a promising one already by\\n Hilbert. The problem, and the intuition behind the proof, is to build a\\n“small” model, one in which the absolute minimum number of\\nreals is allowed, while at the same time the model is large enough to\\nbe closed under all the operations the axioms assert to\\nexist.  \\n[ ]  \\n15  \\nZF  \\nGödel’s is a relative consistency proof, obtained by\\nconstructing a so-called “inner model” for together with the . An inner model is a subcollection of the collection of all sets (see below) which\\nsatisfies the axioms of when only sets in are\\nconsidered. Gödel’s inner model is called the (see below) and is denoted by . Whatever is true in an inner model is consistent with for the same reason that any theory with a model is\\nconsistent. An artifact of the construction is that the Axiom of\\nChoice (henceforth ) is satisfied in Gödel’s\\ninner model and hence the consistency of the with was established by Gödel. Later on it was shown by\\nSierpinski that the is actually a consequence of the\\nGeneralized Continuum Hypothesis or the which states\\nthat for each κ, 2 = κ (see\\nSierpinski 1947).  \\nZF  \\nCH  \\nM  \\nV  \\nZF  \\nM  \\ninner\\nmodel of constructible sets  \\nL  \\nZF  \\nAC  \\nAC  \\nZF  \\nAC  \\nGCH,  \\nκ  \\n+  \\nGödel published two versions of these theorems, in 1939 and in\\n1940, entitled “Consistency Proof for the Generalized Continuum\\nHypothesis,” and “The Consistency of the Axiom of Choice\\nand of the Generalized Continuum Hypothesis with the Axioms of Set\\nTheory,” respectively. Though completely definitive, the 1939\\nversion is lacking in a great many details, most notably the arguments\\nshowing that if is built inside itself, the same results; that is to say, the so-called absoluteness\\narguments are missing. Also missing are the details of the proofs that\\nthe axioms hold in . Unlike the case of the\\nSecond Incompleteness Theorem, however, Gödel subsequently gave a\\ncompletely detailed proof of the two theorems in the 1940 monograph.\\n(The 1940 proof differs substantially from the first version. For\\ndetails about the two proofs and the difference between them the\\nreader is referred to Solovay 1990 and Kanamori 2006.)  \\nL  \\nL  \\nL  \\nZF  \\nL  \\nWe now sketch the proof of the consistency of and of with , using modern terminology. Some\\npreliminary concepts before sketching the proof: We first define the\\nstratified set theoretic universe, denoted . ( is\\nalso known as the cumulative hierarchy.) It is obtained by iteration\\nof the power set operation (℘) beginning with the null set:  \\nCH  \\nAC  \\nZFC  \\nV  \\nV  \\nV  \\n0  \\n=  \\n∅,  \\nV  \\nα+1  \\n=  \\n℘( ),  \\nV  \\nα  \\nV  \\nγ  \\n=  \\n,  \\n∪  \\nβ<γ  \\nV  \\nβ  \\nwhere α, β are any ordinals, γ is a limit ordinal and\\n℘( ) denotes the power set of . Finally  \\nx  \\nx  \\nV  \\n=  \\n,  \\n∪  \\nα∈  \\nOrd  \\nV  \\nα  \\nwhere denotes the class of all ordinals.  \\nOrd  \\nThe constructible hierarchy is likewise defined by\\nrecursion on ordinals. But whereas the full power set operation is\\niterated to obtain the cumulative hierarchy, the levels of the\\nconstructible hierarchy are defined strictly predicatively, that is by\\nincluding at the next level only those sets which are first order\\ndefinable using parameters from the previous level. More exactly, let ( ) denote the set of all subsets of definable in the structure < , ∈ > by first order\\nformulas with parameters in . (For more on definability see\\nthe entry on in this encyclopedia.)  \\nL  \\nDef  \\nA  \\nA  \\nA  \\nA  \\nmodel theory  \\nWith this notation the constructible hierarchy is defined by induction\\nover the ordinals as follows:  \\nL  \\n0  \\n=  \\n∅,  \\nL  \\nα+1  \\n=  \\n( ),  \\nDef  \\nL  \\nα  \\nL  \\nγ  \\n=  \\n,  \\n∪  \\nα<γ  \\nL  \\nα  \\nL  \\n=  \\n,  \\n∪  \\nα∈  \\nOrd  \\nL  \\nα  \\nA set is said to be if ∈ . The axiom which states that all sets are\\nconstructible is denoted = and is called the\\nAxiom of Constructibility. Note that is a proper class and\\nnot a set; although as we will see, each is a set, and the predicate “ is constructible”\\nis actually a definable term of the language.  \\nx  \\nconstructible  \\nx  \\nL  \\nV  \\nL  \\nL  \\nL  \\nα  \\nx  \\nOur next task is to show that is a model of . A\\nset or a class is if elements of it are also\\nsubsets. By a meticulous transfinite induction, can be shown to be transitive for each α; and therefore\\nso is itself. This fact, together with the observation that\\nsome elementary closure properties hold in is enough to show that is a model of . (Indeed,\\nas it turns out, is the minimal transitive model of the axioms containing all the ordinals, and is therefore in\\nthis sense canonical.)  \\nL  \\nZF  \\ntransitive  \\nL  \\nα  \\nL  \\nL  \\n[ ]  \\n16  \\nL  \\nZF  \\nL  \\nZF  \\nIn detail, proving that the axioms, apart from the\\ncomprehension axiom, are true in , amounts to showing that,\\nroughly speaking, any set with a property that a axiom asserts to exist, can be seen to exist in by considering the relativization of the\\nproperty to . (A property is\\nrelativized to an inner model by replacing every quantifier\\n∃ φ by ∃ ( ∈ ∧ φ) and every quantifier ∀ φ\\nby ∀ ( ∈ → φ).) As\\nfor the comprehension axiom, verifying it requires showing that the\\nset asserted to exist is constructed at a particular successor level . Proving this requires an important\\nprinciple of set theory which in modern terminology is called the Levy\\n(or ) Reflection Principle. This principle says that any\\nstatement in the language of which is true in is already true on some level of any continuously increasing hierarchy\\nsuch as . (For the history of this principle, see Kanamori\\n2006.) The Levy Reflection Principle gives the level α at which\\nthe elements of the set are all constructed. Gödel did not\\nactually have the Levy Reflection Principle but used the argument\\nbehind the proof of the principle.  \\nZF  \\nL  \\nP  \\nZF  \\nL  \\nP  \\nL  \\nP  \\nL  \\nP  \\nM  \\nx  \\nx  \\nx  \\nM  \\nx  \\nx  \\nx  \\nM  \\nL  \\nα + 1  \\nZF  \\nZF  \\nV  \\nL  \\nOnce it is established that is a model of , one\\ncan now prove that both the and the hold in . To this end, one first shows that the definition of is for , where absoluteness is\\ndefined as follows: given a class , a predicate ( ) is said to be absolute for if and\\nonly if for all ∈ , ( )\\n↔ ( ).  \\nL  \\nZF  \\nCH  \\nAC  \\nL  \\nL  \\nabsolute  \\nL  \\nM  \\nP  \\nx  \\nM  \\nx  \\nM  \\nP  \\nx  \\nP  \\nM  \\nx  \\nProving that the predicate “ is constructible”\\nis absolute requires formalizing the notion of definability, which in\\nturn requires formalizing the notion of satisfaction. This is because\\nthe predicate “ is constructible” says of a set,\\nthat for some ordinal α, and for some formula φ with\\nparameters in , = { ∈ | ⊨ φ( )}. This part of the proof is tedious but\\nunproblematic.  \\nx  \\nx  \\nL  \\nα  \\nx  \\ny  \\nL  \\nα  \\nL  \\nα  \\ny  \\nOnce the absoluteness of is established, it follows that satisfies the axiom of constructibility if it is\\nrelativized to ; that is, ⊢\\n(V=L) . In particular, the axiom = is\\nconsistent if is.  \\nL  \\nZF  \\nL  \\nZF  \\nL  \\nV  \\nL  \\nZF  \\nWe now give the idea of the proof of and in + = . (For a detailed exposition of\\nthe proof, the reader is referred to the standard sources. See for\\nexample Devlin’s chapter on constructibility in Barwise 1977;\\nsee also Kunen 1983, and Jech 2003.)  \\nCH  \\nAC  \\nZF  \\nV  \\nL  \\nAs concerns the , the idea behind the proof of it in is simply the following: Gödel showed that assuming = , every real number occurs on some countable\\nlevel of the -hierarchy. Since every countable level is\\nitself countable (after all, there are only countably many possible\\ndefining formulas), and there are ω countable\\nlevels, there must be only ω real numbers.  \\nCH  \\nL  \\nV  \\nL  \\nL  \\n1  \\n1  \\nThe difficulty here, if not of the whole proof altogether, lies in\\nshowing that every real is constructed already on a countable level of\\nthe -hierarchy. To show this Gödel argued as follows:\\nSuppose is a real number thought of as a set of natural\\nnumbers. By a combination of the Levy Reflection principle and the\\nLöwenheim-Skolem Theorem there is a countable submodel < , ∈ > of < , ∈ > satisfying a\\nsufficiently large part of the axioms + = , such that belongs to .\\nBy a simple procedure < , ∈ > can be converted\\ninto a transitive model < , ∈ >. This procedure,\\nused by Gödel already in 1937, was explicitly isolated by\\nMostowski (Mostowski 1949). The resulting model is referred to as the\\nMostowski Collapse.  \\nL  \\nA  \\nM  \\nL  \\nfinite  \\nZF  \\nV  \\nL  \\nA  \\nM  \\nM  \\nN  \\nLet us pause to discuss this important technique. Suppose < , > is a well-founded model of the axiom of\\nextensionality. It is a consequence of the well-foundedness of the\\nbinary predicate on , and of the principle of\\ntransfinite recursion, that the equation π( ) =\\n{π( )\\u2009| ∈ ∧ } defines a unique function on . The range of π is transitive, for if π( ) ∈ and ∈ π( ), then =\\nπ( ) for some ∈ with , whence π( ) ∈ . The fact that\\nπ is an isomorphism between < , > and\\n< , ∈ > can be proved by transfinite induction on\\nelements on , based again on the well-foundedness of . The well-foundedness of < , > is\\nin practice often the consequence of < , >\\nbeing a submodel of some < , ε\\n>.  \\nM  \\nE  \\nE  \\nM  \\nx  \\ny  \\ny  \\nM  \\nyEx  \\nM  \\nN  \\na  \\nN  \\ny  \\na  \\ny  \\nb  \\nb  \\nM  \\nbEa  \\nb  \\nN  \\nM  \\nE  \\nN  \\nM  \\nE  \\nM  \\nE  \\nM  \\nE  \\nV  \\nα  \\nWe now return to the proof of the in . We used\\nthe Mostowski Collapse to construct the transitive set . As\\nit turns out, the real number is still an element of < , ∈ > . By basic properties of , < , ∈ > must be < ,\\n∈ > for some α . Since is countable, α\\nis countable too. (It can be shown that | |\\n= |α| + ℵ .) Thus is constructible\\non a countable level, which was to have been shown.  \\nCH  \\nL  \\nN  \\nA  \\nN  \\nL  \\nN  \\nL  \\nα  \\nN  \\nL  \\nα  \\n0  \\nA  \\nAs for the , Gödel exhibits a definable well-ordering,\\nthat is, a formula of set theory which defines, in , a\\nwell-ordering of all of . The formula is tedious to write\\ndown but the idea is a simple one: A set precedes a set in the well-ordering if and only if either occurs in the -hierarchy on an earlier level than , or else they occur on\\nthe same level but is defined by a shorter formula than , or else they are defined by the same formula but the\\nparameters in the definition of occur in earlier\\nthan the parameters of . This well-ordering of shows that the holds in .  \\nAC  \\nL  \\nL  \\nx  \\ny  \\nx  \\nL  \\nL  \\nα  \\ny  \\nx  \\ny  \\nx  \\nL  \\ny  \\nL  \\nAC  \\nL  \\nThis concludes the proof of the consistency of and the in .  \\nAC  \\nCH  \\nL  \\nWe note that Gödel proved more in his 1939 and 1940 than what was\\nshown here, namely he proved the Generalized Continuum Hypothesis in and hence that its consistency with .  \\nL  \\nZF  \\n2.4.3 Consequences of Consistency  \\nAs noted above, it was suggested already in the 1920s that the might be independent of or . After\\nfirst conjecturing that the Axiom of Constructibility might be\\n“absolutely consistent,” meaning not falsifiable by any\\nfurther extension of models of + = , in his 1947 “What is Cantor’s Continuum\\nHypothesis?” Gödel conjectured that the would\\nbe shown to be independent. The main consequence of Gödel’s\\nresult, then, as far as the problem of proving the independence of the is concerned, was that it pointed mathematicians in the\\ndirection of adding non-constructible sets to a model of set theory in\\norder to establish the consistency of the negation of the .\\nIn 1961 Dana Scott proved that the failure of the Axiom of\\nConstructibility follows from the existence of a measurable cardinal,\\ncontrary to a conjecture Gödel had made in 1940. (See Scott 1961.\\nA cardinal κ is said to be measurable if there is a\\nnon-principal κ-complete ultrafilter in the power-set Boolean\\nalgebra of κ.) In 1963, as noted, Paul Cohen proved the\\nconsistency of the negation of the by adding\\nnon-constructible sets to an inner model.  \\nCH  \\nZF  \\nZFC  \\nZF  \\nV  \\nL  \\n[ ]  \\n17  \\nCH  \\nCH  \\nCH  \\nCH  \\nWhat other open questions of set theory could be solved by\\nGödel’s method? Gödel himself noted some consequences.\\nThey are related to so called projective sets of real numbers and\\nfinite sequences of real numbers. The simplest projective sets are the\\nclosed sets, also called Π -sets. A set is\\nΣ if it is the projection of\\na Π -subset of the real plane. A\\nset is Δ if it and its\\ncomplement are Σ . Gödel\\nobserved that there is both a non-Lebesgue measurable\\nΔ -set and an uncountable\\nΠ -set without a perfect subset in . (A set of reals is perfect if it is closed, non-empty, and\\nhas no isolated points. Such sets have the size of the continuum.)\\nGödel gave a sketch of the proof in the 1951 second printing of\\nGödel 1940.  \\n1  \\n0  \\n1  \\n+1  \\nn  \\n1  \\nn  \\n1  \\n+1  \\nn  \\n1  \\n+1  \\nn  \\n1  \\n2  \\n1  \\n1  \\nL  \\nIt has turned out subsequently that the axiom = gives a virtually complete extension of . This means that,\\napart from sentences arising from Gödel’s incompleteness\\ntheorems, essentially all set-theoretical questions can be decided by\\nmeans of the axioms = . This is not to imply that\\nsuch results are in any way trivial. Indeed, it has turned out that is quite a complicated structure, despite its relatively\\nsimple description. As for settling open set-theoretical questions in the main step was the emergence of Jensen’s fine\\nstructure theory of (Jensen 1972). Recalling that the\\nsuccessor step in the definition of\\nthe constructible hierarchy adds to all subsets of definable by first order formulas φ\\nover ( , ∈), fine structure theory,\\nroughly speaking, ramifies the step from to into smaller steps according to the\\ncomplexity of the defining formula φ. Jensen established by means\\nof his fine structure a strengthening, denoted by ◊, of , that he used to construct a Souslin tree in ,\\nand a combinatorial principle □ that he used to show that the\\nSouslin Hypothesis is consistent with .  \\nV  \\nL  \\nZFC  \\nV  \\nL  \\nL  \\nL,  \\nL  \\nL  \\nα +1  \\nL  \\nL  \\nα  \\nL  \\nα  \\nL  \\nα  \\nL  \\nα+1  \\nCH  \\nL  \\nCH  \\n2.4.4 Gödel’s view of the Axiom of Constructibility  \\nIf he did not think this way from the outset, Gödel soon came to\\nadopt the view that the Axiom of Constructibility was implausible. As\\nhe remarked at the end of his 1947 “What is Cantor’s\\nContinuum Hypothesis?”  \\n…it is very suspicious that, as against the numerous plausible\\npropositions which imply the negation of the continuum hypothesis, not\\none plausible proposition is known which would imply the continuum\\nhypothesis. (Gödel 1990, p. 186)  \\nGödel was compelled to this view of by the\\n Leibnizian idea that, rather than the universe being “small,” that\\nis, one with the minimum number of sets, it is more natural to think\\nof the set theoretic universe as being as large as\\n possible. This\\n idea would be reflected in his interest in maximality principles,\\ni.e., principles which are meant to capture the intuitive idea that\\nthe universe of set theory is maximal in the sense that nothing can be\\nadded; and in his conviction that maximality principles would\\neventually settle statements like the . As Gödel put\\nit in a letter to Ulam in the late 1950s, about a maximality principle\\nof von Neumann:  \\nL  \\n[ ]  \\n18  \\n[ ]  \\n19  \\nCH  \\nThe great interest which this axiom has lies in the fact that it is a\\nmaximality principle, somewhat similar to Hilbert’s axiom of\\ncompleteness in geometry. For, roughly speaking, it says that any set\\nwhich does not, in a certain well defined way, imply an inconsistency\\nexists. Its being a maximum principle also explains the fact that this\\naxiom implies the axiom of choice. I believe that the basic problems\\nof set theory, such as Cantor’s continuum problem, will be\\nsolved satisfactorily only with the help of stronger axioms of this\\nkind, which in a sense are opposite or complimentary to the\\nconstructivistic interpretation of mathematics. (Ulam 1958, as quoted\\nin Gödel 1990, p. 168; original emphasis. Note that this is\\ndifferent from the very similar passage Gödel 2003b, p.295.)  \\nTwenty years earlier, in 1938, Gödel had written seemingly\\ndifferently about the Axiom of Constructibility:  \\nThe proposition (i.e., = ) added as a\\nnew axiom seems to give a natural completion of the axioms of set\\ntheory, in so far as it determines the vague notion of an arbitrary\\ninfinite set in a definite way. (Gödel 1986, p.27)  \\nA  \\nV  \\nL  \\nGödel may have meant by “natural completion” here\\n“the correct completion,” or he may have meant to say no\\nmore than that the Axiom of Constructibility determines the notion of\\nset in a definite way. In any case he used the term\\n“natural” differently in a conversation with Wang on\\nconstructibility in 1972 (Wang 1996, p. 144):  \\nGödel talked more about the relation between axioms of infinity\\nand the constructible universe…(he observed that) preliminary\\nconcepts such as that of constructible sets are necessary to arrive at\\nthe natural concept, such as that of set.  \\nThis is reminiscent of a remark of Hugh Woodin, that studying forcing\\nleads to a better understanding of — the general\\nprinciple being that studying the models of a theory is not only\\nuseful to understand the theory itself, but useful to obtain a better\\npicture of (Woodin 1988).  \\nV  \\nV  \\nFor more on Gödel’s program and on Gödel’s\\nprogram relative to the the reader is referred e.g., to\\nSteel forthcoming and Feferman . 2000. For more on\\nGödel’s result, its history , and its significance the\\nreader is referred to Floyd/Kanamori 2006 and Kennedy 2006.  \\nCH  \\net al  \\n2.5 Gödel’s Work in Intuitionistic Logic and Arithmetic  \\nGödel’s interest in intuitionism was deep and long-lasting.\\nAlthough he himself did not subscribe to that view, he made a number\\nof important contributions to intuitionistic logic. Perhaps the\\nimportance he placed on the concept of evidence (see below) led to his\\nclose consideration of it.  \\nWe discuss Gödel’s results on intuitionistic logic in their\\nchronological order.  \\n2.5.1 Intuitionistic Propositional Logic is not Finitely-Valued  \\nBoth many-valued logic, introduced by Łukasiewicz in the twenties\\n(Łukasiewicz 1970) and intuitionistic logic, formalized by\\nHeyting in 1930, fail to satisfy the law of excluded middle. It was\\ntherefore natural to ask whether intuitionistic logic can be presented\\nas a many-valued logic, and indeed a number of logicians in the 1920s\\nhad suggested just that. In his 1932 Gödel gave a simple argument\\nwhich shows that intuitionistic propositional logic cannot be thought\\nof as a finitely-valued logic. Precisely, Gödel proved two\\ntheorems:  \\n. There is no realization with finitely many elements (truth values) for\\nwhich the formulas provable in , and only those, are\\nsatisfied (that is, yield designated values for an arbitrary\\nassignment).  \\nTheorem 7  \\nH  \\n( is intuitionistic propositional logic, after\\nHeyting.)  \\nH  \\n. Infinitely many systems lie between and the system of the ordinary propositional calculus, that is,\\nthere is a monotonically decreasing sequence of systems all of which\\ninclude as a subset and are included in as subsets.  \\nTheorem 8  \\nH  \\nA  \\nH  \\nA  \\nIn his proof he considered for each natural number > 0\\nthe sentence  \\nn  \\n= ≡ .  \\nF  \\nn  \\n∨  \\n1\\n≤ < ≤  \\ni  \\nj  \\nn  \\np  \\ni  \\np  \\nj  \\nHe observed that in an -valued logic the sentences , for > ,\\nshould be derivable. However, Gödel showed, is not derivable from Heyting’s\\naxioms for any .  \\nn  \\nF  \\nm  \\nm  \\nn  \\nF  \\nn  \\nn  \\nSubsequently Jaśkowski (Jaśkowski 1936) showed that\\nintuitionistic propositional logic can be given a many-valued\\nsemantics in terms of infinitely many truth-values. For further\\ndiscussion of many-valued logics, see for example the entry on in this encyclopedia as well as van Stigt’s article on\\nintuitionistic logic in Mancosu 1998.  \\nmany-valued logic  \\n2.5.2 Classical Arithmetic is Interpretable in Heyting Arithmetic  \\nWe now consider Gödel 1933e, in which Gödel showed, in\\neffect, that intuitionistic or Heyting arithmetic is only apparently\\nweaker than classical first-order arithmetic. This is because the\\nlatter can be interpreted within the former by means of a simple\\ntranslation, and thus to be convinced of the consistency of classical\\narithmetic, it is enough to be convinced of the consistency of Heyting\\narithmetic. Heyting arithmetic is defined to be the same as classical\\narithmetic, except that the underlying predicate logic is given by\\nintuitionistic axioms and rules of inference (see below).  \\nThis result extends the same assertion for the propositional case. Let denote the intuitionistic propositional logic, and denote its classical counterpart (as above).\\nInductively define:  \\nH  \\nA  \\n′  \\nA  \\n≡  \\n¬¬ ( atomic)  \\nA  \\nA  \\n(¬ )′  \\nA  \\n≡  \\n¬ ′  \\nA  \\n( → )′  \\nA  \\nB  \\n≡  \\n¬( ′ ∧\\n¬ ′)  \\nA  \\nB  \\n( ∨ )′  \\nA  \\nB  \\n≡  \\n¬(¬ ′ ∧\\n¬ ′)  \\nA  \\nB  \\n( ∧ )′  \\nA  \\nB  \\n≡  \\n′ ∧ ′  \\nA  \\nB  \\nThen,  \\n. Let be a propositional formula. Then ⊢ if and only if ⊢ ′,  \\nTheorem 9  \\nF  \\nH  \\nF  \\nA  \\nF  \\nThe theorem follows easily from the result of Glivenko (1929) that\\n¬ follows from if and only if\\n¬ follows from , for any propositional\\nformula .  \\nF  \\nH  \\nF  \\nA  \\nF  \\nGödel’s so-called double negation interpretation extends\\nTheorem 9 to a reduction of classical first order logic to\\nintuitionistic predicate logic. The translation in this case can be\\ntaken to map ′ to for atomic .\\nMoreover, we let ∀ ( )′ =\\n∀ ′( ) :  \\nA  \\nA  \\nA  \\nxA  \\nx  \\nxA  \\nx  \\n. Suppose is a first order formula. If is provable\\nin classical first order logic, then ′ is provable in\\nintuitionistic first order logic.  \\nTheorem 10  \\nA  \\nA  \\nA  \\nThe above result had been obtained independently by Gentzen (with\\nBernays), but upon hearing of Gödel’s result Gentzen\\nwithdrew his paper from publication. It had also been anticipated by\\nKolmogorov in his 1925 “On the Principle of the Excluded\\nMiddle,” (English translation van Heijenoort 1967) but that\\npaper was largely unknown to logicians who were outside of\\nKolmogorov’s circle.  \\nBernays has written (see Bernays’ entry on David Hilbert in\\nEdwards 1967) that this result of Gödel’s drew the\\nattention of the Hilbert school to two observations: first, that\\nintuitionistic logic goes beyond finitism, and secondly, that finitist\\nsystems may not be the only acceptable ones from the foundational\\npoint of view.  \\nThe following theorem for the case of arithmetic follows from Theorem\\n10:  \\n. Suppose is a first order formula of arithmetic. If is provable in classical Peano arithmetic, then ′ is provable in intuitionistic first order\\narithmetic.  \\nTheorem 11  \\nA  \\nA  \\nA  \\nFor a list of the axioms and rules of intuitionistic first order logic\\nsee Gödel 1958, reprinted with detailed introductory note by A.S.\\nTroelstra in Gödel 1990. See also Troelstra 1973, and\\nTroelstra’s “Aspects of constructive mathematics” in\\nBarwise 1977. For a detailed proof of the above theorem the reader is\\nreferred also to the latter.  \\n2.5.3 Intuitionistic Propositional Logic is Interpretable in  \\nS4  \\nThis result of Gödel’s (Gödel 1933f), which marks the\\nbeginning of provability logic, makes exact the difference between the\\nconcept of “provability in a specified formal system” and\\nthat of “provability by any correct means.”  \\nGödel had already noted this difference in the introduction to\\nhis 1929 thesis. The context was the following: Gödel entertains\\nthere the possibility that his proof of the Completeness Theorem might\\nbe circular, since the law of excluded middle was used to prove it.\\nThis is because while the Completeness Theorem asserts ‘a kind\\nof decidability,’ namely every quantificational formula is\\neither provable or a counterexample to it can be given, ‘the\\nprinciple of the excluded middle seems to express nothing other than\\nthe decidability of every problem’:  \\n… what is affirmed (by the law of excluded middle) is the\\nsolvability not at all through specified means but only through all\\nmeans that are …  \\nin any way imaginable  \\n[ ]  \\n20  \\nGödel considers intuitionistic propositional logic (henceforth\\nIPL); he also considers a second system, classical propositional logic\\nenriched by an operator “B”, where the intended meaning of\\n“B” is “provable.” The axiom system now known\\nas (for a list of these axioms see for example the\\nentry on in this encyclopedia) is added to the standard axioms for classical\\npropositional logic together with a new rule of proof: from , B may be inferred. Let us call this second\\nsystem . Gödel’s theorem states that is interpretable in via the following\\ntranslation:  \\nS4  \\nmodal logic  \\nA  \\nA  \\nG  \\nIPL  \\nG  \\n¬  \\np  \\n≡  \\n~B  \\np  \\n⊃  \\np  \\nq  \\n≡  \\nB → B  \\np  \\nq  \\n∨  \\np  \\nq  \\n≡  \\nB ∨ B  \\np  \\nq  \\n∧  \\np  \\nq  \\n≡  \\nB ∧ B  \\np  \\nq  \\nThat is,  \\n. Let be a formula of , and let ′\\nbe its translation. Then ⊢ implies ⊢ ′.  \\nTheorem 12  \\nA  \\nIPL  \\nA  \\nIPL  \\nA  \\nG  \\nA  \\nGödel conjectures that the converse implication must be true, and\\nindeed this was shown in McKinsey and Tarski 1948.  \\nThe difference between the two notions of provability: “provable\\nin a given formal system ” and provability by any\\ncorrect means — manifests itself as a consequence of\\nGödel’s Second Incompleteness Theorem, as follows. Let contain Peano arithmetic, and let the operator B be\\ninterpreted as “provable in ”. If the axioms of were valid for this interpretations of ,\\nthen from (0 ≠ 1) → (0 ≠ 1), the sentence\\n¬ (0 ≠ 1) would be provable, contradicting the Second\\nIncompleteness Theorem.  \\nS  \\nS  \\nS  \\nS4  \\nB  \\nB  \\nB  \\nFor further discussion of Gödel’s theorem, its antecedents\\nand its extensions, as well as its philosophical significance, the\\nreader is referred to A.S Troelstra’s introduction to .  \\n1933f  \\n2.5.4 Heyting Arithmetic is Interpretable into Computable Functionals of Finite Type.  \\nGödel’s so-called Dialectica intepretation (Gödel\\n1958) delivers a relative consistency proof and justification for\\nHeyting arithmetic by means of a concrete interpretation involving a\\nsystem of computable functionals of finite type. Taken\\ntogether with his 1933e, which reduces classical first order\\narithmetic to Heyting arithmetic, a justification in these terms is\\nalso obtained for classical first order arithmetic.  \\nT  \\nGödel’s inductive definition of the notion “function\\nof finite type” is as follows: (Gödel 1990, p. 245).  \\nThe functionals of type 0 are the natural numbers.  \\nIf ,…, are types and we have already defined\\nwhat functionals of types ,…, are,\\nthen ( ,…, ) is a type and a functional of that\\ntype assigns to every -tuple of functionals of respective\\ntypes ,…, , a functional of type .  \\nt  \\n0  \\nt  \\nk  \\nt  \\n0  \\nt  \\nk  \\nt  \\n0  \\nt  \\nk  \\nk  \\nt  \\n1  \\nt  \\nk  \\nt  \\n0  \\nGödel considers the quantifier free theory of these functionals\\nof finite type, denoted by . has the following\\nfeatures: the language of contains variables of each type,\\nconstants for distinguished types, and a ternary predicate\\n= for equality for type σ. Equality between\\nterms of the same type is decidable. The non-logical axioms and rules\\nfor include the classical arithmetic axioms for 0 and\\nsuccessor, and the induction rule:  \\nT  \\nT  \\nT  \\nσ  \\nT  \\n( (0) ∧ ( ( ) → ( ( )))) → ( )  \\nF  \\nF  \\nx  \\n0  \\nF  \\nS  \\nx  \\n0  \\nF  \\nx  \\n0  \\nfor quantifier-free formulas ( ). As\\nGödel remarks (Gödel 1990, p. 247), the axioms for are essentially those of primitive recursive arithmetic,\\nexcept that the variables can be of any finite type.  \\nF  \\nx  \\n0  \\nT  \\nGödel’s translation associates with every formula ( ) of the language of Peano arithmetic a\\nformula ′( ) =\\n∃ ∀ ( , , ) of the language of the theory , where is quantifier free and the (boldface)\\nbound variables are finite sequences of variables thought to range\\nover functionals of a finite type determined by the type of the\\nvariable. Intuitively, is a concrete analogue of\\nthe abstract notion of a construction constituting the meaning of .  \\nF  \\nx  \\nF  \\nx  \\ny  \\nz  \\nA  \\ny  \\nz  \\nx  \\nT  \\nA  \\ny  \\nF  \\nGödel’s theorem is as follows:  \\n. Suppose ′ =\\n∃ ∀ ( , , ). If is provable in\\nintuitionistic first order arithmetic, then there are computable\\nfunctionals of finite type such that ( ( ), , ) is provable in .  \\nTheorem 13  \\nF  \\ny  \\nz  \\nA  \\ny  \\nz  \\nx  \\nF  \\nQ  \\nA  \\nQ  \\nx  \\nz  \\nx  \\nT  \\nThe proof is by induction on the structure of the proof of in intuitionistic first order arithmetic. (For a treatment of the\\nproof in detail, the reader is referred to Troelstra 1986.)  \\nF  \\nThe importance of the theorem for foundations cannot be\\n overstated. A discussion of its generalizations, of ensuing work on functional\\ninterpretations stimulated by the theorem due to Kreisel, Tait,\\nHoward, Feferman and others; its foundational and philosophical\\nsignificance; and finally its relation particularly to the earlier,\\ninformal, proof interpretation, so-called, given by\\nHeyting-Kolmogorov, will not be attempted here. Accordingly the reader\\nis referred to the large literature on the subject, e.g., the\\nabovementioned Troelstra 1986, Tait 1967, Feferman 1993 and Avigad\\n& Feferman 1998. For interesting recent developments, e.g., in the\\narea of relating Gödel’s Dialectica interpretation and\\nKreisel’s modified realizability, see Oliva 2006. See also van\\nOosten 2008.  \\n[ ]  \\n21  \\nA remark concerning the philosophical context in which Gödel\\npresented his translation, namely finitism. The question addressed in\\nthe introduction to the paper is what notions must\\nbe added to finitary mathematics in order to obtain a consistency\\nproof for arithmetic. Equivalently: what does the finitary view\\npresuppose, which must be given up in the light of the Second\\nIncompleteness Theorem, if the consistency proof is to be\\nobtained:  \\nabstract  \\nIn any case Bernays’ remark teaches us to distinguish two\\ncomponents of the finitary attitude; namely, first, the constructive\\nelement, which consists in our being allowed to speak of mathematical\\nobjects only insofar as we can exhibit them or actually produce them\\nby means of a construction; second, the specifically finitistic\\nelement, which makes the further demand that the objects about which\\nwe make statements, with which the constructions are carried out and\\nwhich we obtain by means of these constructions, are\\n‘intuitive’, that is, are in the last analysis\\nspatiotemporal arrangements of elements whose characteristics other\\nthan their identity or nonidentity are irrelevant.… It is the\\nsecond requirement that must be dropped. This fact has hitherto been\\ntaken into account by our adjoining to finitary mathematics parts of\\nintuitionistic logic and the theory of ordinals. In what follows we\\nshall show that, for the consistency proof of number theory, we can\\nuse, instead, the notion of computable function of finite type on the\\nnatural numbers and certain rather elementary principles of\\nconstruction for such functions. (Gödel 1990, p.245).  \\nAside from its technical contribution, then, Gödel’s\\n1958/72 is one of Gödel’s most important philosophical\\nworks; notable for its analysis of the nature of finitary mathematics,\\nas well as its analysis of the notions of “intuitive,” as\\nin “intuitive knowledge,” and that of abstract versus\\nconcrete evidence.  \\nIn the next section, we turn to Gödel’s philosophical\\nviews. But interested readers may wish to read a brief discussion\\nabout Gödel’s Nachlass, important source of philosophical\\nmaterial by Gödel:  \\nSupplement Document: Gödel’s Documents  \\n3. Gödel’s Philosophical Views  \\nGödel’s philosophical views can be broadly characterized by\\ntwo points of focus, or, in modern parlance, commitments. These are:\\nrealism, namely the belief that mathematics is a descriptive science\\nin the way that the empirical sciences are. The second commitment is\\nto a form of Leibnizian rationalism in philosophy; and in fact\\nGödel’s principal philosophical influences, in this regard\\nparticularly but also many others, were Leibniz, Kant and Husserl.\\n(For further discussion of how these philosophers influenced\\nGödel, see van Atten and Kennedy 2003.)  \\nThe terms “Gödel’s realism” and\\n“Gödel’s rationalism” must be prefaced with a\\ndisclaimer: there is no single view one could associate with each of\\nthese terms. Gödel’s realism underwent a complex\\ndevelopment over time, in both the nature of its ontological claims as\\nwell as in Gödel’s level of commitment to those claims.\\nSimilarly Gödel’s rationalism underwent a complex\\ndevelopment over time, from a tentative version of it at the\\nbeginning, to what was adjudged to be a fairly strong version of it in\\nthe 1950s. Around 1959 and for some time afterward Gödel fused\\nhis rationalistic program of developing exact philosophy with the\\nphenomenological method as developed by Husserl.  \\nWe examine these two strains of Gödel’s thinking below:  \\n3.1 Gödel’s Rationalism  \\nGödel’s rationalism has its roots in the Leibnizian thought\\nthat the world, not that which we immanently experience but that which\\nitself gives rise to immanent experience, is perfect and beautiful,\\nand therefore rational and ordered. Gödel’s justification\\nof this belief rests partly on an inductive generalization from the\\nperfection and beauty of mathematics:  \\nRationalism is connected with Platonism because it is directed to the\\nconceptual aspect rather than toward the (real) world. One uses\\ninductive evidence…Mathematics has a form of\\nperfection…We may expect that the conceptual world is perfect,\\nand, furthermore, that objective reality is beautiful, good, and\\nperfect. (Wang 1996, 9.4.18)  \\nOur total reality and total experience are beautiful and\\nmeaningful—this is also a Leibnizian thought. We should judge\\nreality by the little which we truly know of it. Since that part which\\nconceptually we know fully turns out to be so beautiful, the real\\nworld of which we know so little should also be beautiful.\\n(9.4.20)  \\nAlthough the roots of Gödel’s belief in rationalism are\\nmetaphysical in nature, his long-standing aspirations in that domain\\nhad always been practical ones. Namely, to develop exact methods in\\nphilosophy; to transform it into an exact science, or , to use Husserl’s term.  \\nstrenge\\nWissenschaft  \\nWhat this means in practice is taking the strictest view possible of\\nwhat constitutes the grounds for the acceptance\\nof an assertion; put another way, a level of rigor is aspired to in\\nphilosophical arguments approaching that which is found in\\nmathematical proofs. A formulation of the view—one which is\\nsomewhat phenomenologically colored (see below)—can be found in\\na document in the Gödel Nachlass. This is a fourteen item list\\nGödel drew up in about 1960, entitled “My Philosophical\\nViewpoint.” Two items on the list are relevant here:  \\ndialectical  \\nThere are systematic methods for the solution of all problems\\n(also art, etc.).  \\nThere is a scientific (exact) philosophy and theology, which deals\\nwith concepts of the highest abstractness; and this is also most\\nhighly fruitful for science.  \\n(The list was transcribed by Cheryl Dawson and was published in , p. 316.)  \\nWang 1996  \\nGödel’s earlier conception of rationalism refers to\\nmathematical rigor and includes the concept of having a genuine proof,\\nand is therefore in some sense a more radical one than that to which\\nhe would later subscribe. One can see it at work at the end of the\\nGibbs lecture, after a sequence of arguments in favor of realism are\\ngiven:  \\nOf course I do not claim that the foregoing considerations amount to a\\nreal proof of this view about the nature of mathematics. The most I\\ncould assert would be to have disproved the nominalistic view, which\\nconsiders mathematics to consist solely in syntactical conventions and\\ntheir consequences. Moreover, I have adduced some strong arguments\\nagainst the more general view that mathematics is our own creation.\\nThere are, however, other alternatives to Platonism, in particular\\npsychologism and Aristotelian realism. In order to establish Platonic\\nrealism, these theories would have to be disproved one after the\\nother, and then it would have to be shown that they exhaust all\\npossibilities. I am not in a position to do this now; however I would\\nlike to give some indications along these lines. (Gödel 1995, p.\\n321–2).  \\n(For a penetrating analysis of this passage see Tait 2001.) Such an\\nanalysis must be based on conceptual analysis:  \\nI am under the impression that after sufficient clarification of the\\nconcepts in question it will be possible to conduct these discussions\\nwith mathematical rigour and that the result will then be…that\\nthe Platonistic view is the only one tenable. (Gödel 1995, p.\\n322).  \\nAlong with the methodological component, as can be seen from the items\\non Gödel’s list, there was also an “optimistic”\\ncomponent to Gödel’s rationalism: once the appropriate\\nmethods have been developed, philosophical problems such as, for\\nexample, those in ethics (e.g., item 9 on the list is: “Formal\\nrights comprise a real science.”) can be decisively solved. As\\nfor mathematical assertions, such as the Continuum Hypothesis in set\\ntheory, once conceptual analysis has been carried out in the right\\nway, that is, once the basic concepts, such as that of\\n“set,” have been completely clarified, the Continuum\\nHypothesis should be able to be decided.  \\nAlthough at the time of the Gibbs lecture the analogy in\\nGödel’s mind between philosophical and mathematical\\nreasoning may have been a very close one, Gödel’s view at\\nother periods was that the envisaged methods will not be mathematical\\nin nature. What was wanted was a general, informal science of\\nconceptual analysis.  \\nPhilosophy is more general than science. Already the theory of\\nconcepts is more general than mathematics…True philosophy is\\nprecise but not specialized.  \\nPerhaps the reason why no progress is made in mathematics (and there\\nare so many unsolved problems), is that one confines oneself to the\\next[ensional]—thence also the feeling of disappointment in the\\ncase of many theories, e.g., propositional logic and formalisation\\naltogether. (Wang 1996, 9.3.20,\\n 9.3.21)  \\n[ ]  \\n22  \\n(See notebook Max IV, p. 198 (Gödel Nachlaß, Firestone\\nLibrary, Princeton, item 030090). Transcription Cheryl Dawson;\\ntranslation from the German ours; amendment ours. Gödel’s\\ndating of Max IV indicates that it is from May 1941 to April 1942. See\\nalso Gödel’s letter to Bernays, Gödel 2003a, p.\\n283.)  \\nAn important source for understanding Gödel’s advance\\ntoward a general theory of concepts are Gödel’s remarks on\\nconceptual analysis published by Hao Wang in .\\nIn remark 8.6.10 for example, Gödel expresses the belief that\\nextensionality fails for concepts, contrary to what he said in his\\n1944 “Russell’s Mathematical Logic,” a remark which\\nhe now wishes to retract:  \\nLogical Journey  \\nI do not (no longer) believe that generally sameness of range is\\nsufficient to exclude the distinctness of two concepts.  \\nIn some of Gödel’s later discussions another component of\\nconceptual analysis emerges, namely the project of finding the\\nso-called primitive terms or concepts, and their relations. These are\\nroughly terms or concepts which comprise a theoretical “starting\\npoint,” on the basis of their meaning being completely definite\\nand clear. For example, the concept of “the application of a\\nconcept to another concept” is a primitive term, along with\\n“force”. (Wang 1996, 9.1.29).  \\nHe spoke to Wang about the general project in 1972:  \\nPhenomenology is not the only approach. Another approach is to find a\\nlist of the main categories (e.g., causation, substance, action) and\\ntheir interrelations, which, however, are to be arrived at\\nphenomenologically. The task must be done in the right manner. (Wang\\n1996, 5.3.7).  \\nGödel spoke with Sue Toledo between 1972 and 1975 about the\\nproject of finding primitive terms, as well as other aspects of\\nphenomenology. See Toledo 2011. We discuss Gödel’s\\ninvolvement with phenomenology further in the supplementary document .  \\nGödel’s Turn to Phenomenology  \\nThe judgement levied upon Gödel’s rationalism by\\ncontemporary philosophers was a harsh one. (See for example Gödel\\n1995, pp. 303–4). Nevertheless Gödel himself remained\\noptimistic. As he commented to Wang:  \\nIt is not appropriate to say that philosophy as rigorous science is\\nnot realizable in the foreseeable future. Time is not the main factor;\\nit can happen anytime when the right idea appears. (Wang 1996,\\n4.3.14).  \\nGödel concluded his 1944 on a similarly optimistic note.  \\n3.2 Gödel’s Realism  \\nGödel’s realist views were formulated mostly in the context\\nof the foundations of mathematics and set theory.  \\nWe referred above the list “What I believe,” thought to\\nhave been written in 1960 or thereabouts. Out of 14 items, only two\\nrefer to realism, remarks 10 and 12:  \\nMaterialism is false.  \\nConcepts have an objective existence.  \\nGödel published his views on realism for the first time in his\\n1944. The following is one of his most quoted passages on the\\nsubject:  \\nClasses and concepts may, however, also be conceived as real objects,\\nnamely classes as “pluralities of things,” or as\\nstructures consisting of a plurality of things and concepts as the\\nproperties and relations of things existing independently of our\\ndefinitions and constructions.  \\nIt seems to me that the assumption of such objects is quite as\\nlegitimate as the assumption of physical bodies and there is quite as\\nmuch reason to believe in their existence. They are in the same sense\\nnecessary to obtain a satisfactory system of mathematics as physical\\nbodies are necessary for a satisfactory theory of our sense\\nperceptions and in both cases it is impossible to interpret the\\npropositions one wants to assert about these entities as propositions\\nabout the “data,” i.e., in the latter case the actually\\noccurring sense perceptions.  \\nGödel’s reference to the impossibility of interpreting\\nempirical laws, or more precisely, instantiations of them—the\\nstatements “one wants to assert,”—as statements\\nabout sense perceptions, is likely an endorsement of the (then)\\ncontemporary critique of phenomenalism. The critique was based on the\\nobservation that sense data are so inextricably bound up with the\\nconditions under which they are experienced, that no correspondence\\nbetween statements about those and the statements “we want to\\nassert” can be given (see Chisholm 1948 for example). More\\ngenerally Gödel was against verificationism, namely the idea that\\nthe meaning of a statement is its mode of verification.  \\nThe analogical point in the first part of the passage was amplified by\\nGödel in the draft manuscript “Is Mathematics a Syntax of\\nLanguage?”:  \\nIt is arbitrary to consider “This is red” an immediate\\ndatum, but not so to consider the proposition expressing modus ponens\\nor complete induction (or perhaps some simpler propositions from which\\nthe latter follows). (Gödel 1995, p. 359)  \\nSome writers have interpreted Gödel in this and similar passages\\npragmatically, attributing to him the view that because empirical\\nstatements are paradigmatic of successful reference, reference in the\\ncase of abstract concepts should be modelled causally. (See Maddy\\n1990.) Interpreting reference to objects this way,\\nit is argued, addresses the main difficulty associated with realism,\\nthe problem how we can come to have knowledge of abstract objects.\\nOthers have argued that Gödel had no paradigm case in mind; that\\nfor him both the empirical and the abstract case are either equally\\nproblematic, or equally unproblematic. (See Tait 1986.) The latter\\nview is referred to as epistemological parity in van Atten and Kennedy\\n2003. (See also Kennedy and van Atten 2004.)  \\nabstract  \\nIn his 1947 “What is Cantor’s Continuum Problem?”,\\nGödel expounds the view that in the case of meaningful\\npropositions of mathematics, there is always a fact of the matter to\\nbe decided in a yes or no fashion. This is a direct consequence of\\nrealism, for if there exists a domain of mathematical objects or\\nconcepts, then any meaningful proposition concerning them must be\\neither true or\\n false. The Continuum Hypothesis is Gödel’s example of a\\nmeaningful question. The concept “how many” leads\\n“unambiguously” to a definite meaning of the hypothesis,\\nand therefore it should be decidable—at least in principle. Most\\nstrikingly Gödel does not leave the matter there but goes on to\\noffer a practical strategy for determining the value of the continuum,\\nas well as the truth value of other axioms extending .\\nSpecifically, he offers two criteria for their decidability: the first\\ninvolves conceptual analysis and is associated with Gödel’s\\nrationalistic program. (See the above section on Gödel’s\\nrationalism.) Secondly one must keep an eye on the so-called success\\nof the axiom, as a check or indicator of which direction to look to\\nfor the solution of its truth. For example, Gödel notes in the\\npaper that none of the consequences of the Axiom of Constructibility\\nare very plausible. It is, then, likely false. See Maddy 2011 and\\nKoellner 2014 for discussion of intrinsic vs extrinsic justifications\\nfor new axioms of set theory.  \\n[ ]  \\n23  \\nZFC  \\nFor further discussion of Gödel’s philosophical views see\\nthe supplementary documents:  \\nGödel’s Turn to Phenomenology  \\nand  \\nA Philosophical Argument About the Content of Mathematics  \\nBibliography  \\nPrimary Sources  \\nGödel’s Writings  \\nThe Gödel Nachlass is located at Firestone Library of Princeton\\nUniversity with the exception of Gödel’s preprint\\ncollection, which is housed at the library of the Institute for\\nAdvanced Study. The Nachlass itself is the property of the Institute\\nbut a microfilm copy of it may be purchased from Brill. All of\\nGödel’s published work, together with a large number of the\\nunpublished material from the Nachlass, together with a selection of\\nGödel’s correspondence is published in .  \\nKurt Gödel,\\nCollected Works, Volumes I-V  \\nThe Collected Papers of Kurt Gödel  \\n1986, .\\nS. Feferman, S. Kleene, G. Moore, R. Solovay, and J. van Heijenoort\\n(eds.), Oxford: Oxford University Press.  \\nCollected Works. I: Publications 1929–1936  \\n1990, .\\nS. Feferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van\\nHeijenoort (eds.), Oxford: Oxford University Press.  \\nCollected Works. II: Publications 1938–1974  \\n1995, . S. Feferman, J. Dawson, S. Kleene, G. Moore, R.\\nSolovay, and J. van Heijenoort (eds.), Oxford: Oxford University\\nPress.  \\nCollected Works. III: Unpublished essays and\\nlectures  \\n2003a, . S.\\nFeferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van\\nHeijenoort (eds.), Oxford: Oxford University Press.  \\nCollected Works. IV: Correspondence A-G  \\n2003b, . S.\\nFeferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van\\nHeijenoort (eds.), Oxford: Oxford University Press.  \\nCollected Works. V: Correspondence H-Z  \\nSelected Works of Kurt Gödel  \\n[1929]  \\n“I”, . Reprinted in Gödel 1986, pp. 60–101.  \\nDissertation, University of\\nVienna  \\n[1930]  \\n“Die Vollständigkeit der Axiome des\\nlogischen Funktionenkalküls”, , 37: 349–360. Reprinted in Gödel\\n1986, pp. 102–123.  \\nMonatshefte für\\nMathematik und Physik  \\n[1931]  \\n“Über formal unentscheidbare Sätze\\nder Principia Mathematica und verwandter Systeme, I”, , 38:\\n173–198. Reprinted in Gödel 1986, pp. 144–195.  \\nMonatshefte für Mathematik und Physik  \\n[1932]  \\n“Zum intuitionistischen\\nAussagenkalkül”, , 69: 65–66. Reprinted in Gödel\\n1986, pp. 222–225.  \\nAnzeiger der Akademie der\\nWissenschaften in Wien  \\n[1933e]  \\n“Zur intuitionistischen Arithmetik und\\nZahlentheorie”, , 4: 34–38. Reprinted in Gödel 1986, pp.\\n286–295.  \\nErgebnisse eines mathematischen\\nKolloquiums  \\n[1933f]  \\n“Eine Interpretation des intuitionistischen\\nAussagenkalküls”, 4, 39–40. Reprinted in Gödel 1986, pp.\\n300–301.  \\nErgebnisse eines mathematischen\\nKolloquiums  \\n[1933i]  \\n“Zum Entscheidungsproblem des logischen\\nFunctionenkalküls”, , 40: 433–443. Reprinted in Gödel 1986, pp.\\n306–326.  \\nMonatshefte für Mathematik und\\nPhysik  \\n[*1933o]  \\n“The present situation in the foundations of\\nmathematics”, manuscript. Printed in Gödel 1995, pp.\\n45–53.  \\n[1934c]  \\nReview of Skolem (1933). , 7: 193–194. Reprinted in\\nGödel 1986, pp. 379–380.  \\nZentralblatt für\\nMathematik und ihre Grenzgebiete  \\n[1936a]  \\n“Über die Länge von\\nBeweisen”, ,\\n7: 23–24. Reprinted in Gödel 1986, pp. 395–399.  \\nErgebnisse eines mathematischen Kolloquiums  \\n[1939a]  \\n“Consistency proof for the generalized\\ncontinuum hypothesis”, , 25: 220–224. Reprinted in Gödel\\n1990, pp. 28–32.  \\nProceedings of the National Academy\\nof Sciences, U.S.A.  \\n[1940]  \\n“The Consistency of the Continuum\\nHypothesis”, , Volume 3,\\nPrinceton: Princeton University Press. Reprinted in Gödel 1990,\\npp. 33–101.  \\nAnnals of Mathematics Studies  \\n[*1941]  \\n“In what sense is intuitionistic logic\\nconstructive?”, lecture manuscript. Printed in Gödel 1995,\\npp. 189–200.  \\n[1944]  \\n“Russell’s mathematical logic”, (Library of Living\\nPhilosophers), P. Schilpp (ed.), New York: Tudor, 1951, pp.\\n123–153. Reprinted in Gödel 1990, pp. 119–141.  \\nThe Philosophy of Bertrand Russell  \\n[*1946/9-B2]  \\n“Some observations about the relationship\\nbetween theory of relativity and Kantian philosophy”,\\nmanuscript. Printed in Gödel 1995, pp. 230–246.  \\n[*1946/9-C1]  \\n“Some observations about the relationship\\nbetween theory of relativity and Kantian philosophy”,\\nmanuscript. Printed in Gödel 1995, pp. 247–259.  \\n[1947]  \\n“What is Cantor’s continuum\\nproblem?”, , 54: 515–525.\\nReprinted in Gödel 1990, pp. 176–187.  \\nAmer. Math. Monthly  \\n[1949a]  \\n“A remark on the relationship between\\nrelativity theory and idealistic philosophy”, (Library of Living Philosophers),\\nP. Schilpp (ed.), La Salle, IL: Open Court, 1949, pp. 555–562.\\nReprinted in Gödel 1990, pp. 202–207.  \\nAlbert\\nEinstein: Philosopher-Scientist  \\n[1949]  \\n“An Example of a New Type of Cosmological\\nSolutions of Einstein’s Field Equations of Gravitation,” , 21: 447–450. Reprinted in\\nGödel 1990, pp. 190–198.  \\nReviews of Modern Physics  \\n[*1951]  \\n“Some basic theorems on the foundations of\\nmathematics and their implications”, lecture manuscript. Printed\\nin Gödel 1995, pp. 304–323.  \\n[*1953/9-III]  \\n“Is mathematics a syntax of language?”,\\nlecture manuscript. Printed in Gödel 1995, pp.\\n334–356.  \\n[*1953/9-V]  \\n“Is mathematics a syntax of language?,”\\nlecture manuscript. Printed in Gödel 1995, pp.\\n356–362.  \\n[1958]  \\n“Über eine bisher noch nicht\\nbenützte Erweiterung des finiten Standpunktes”, , 12: 280–287. Reprinted in Gödel 1990,\\npp. 240–251.  \\nDialectica  \\n[*1961/?]  \\n“The modern development of the foundations of\\nmathematics in light of philosophy”, manuscript. Printed in\\nGödel 1995, pp. 374–387.  \\n[1964]  \\n“What is Cantor’s continuum\\nproblem? , revised version of Gödel 1947, in\\nBenacerraf, P. and Putnam, H. (eds.), 1983, , Cambridge: Cambridge\\nUniversity Press. Reprinted in Gödel 1990, pp.\\n254–270.  \\n”  \\nPhilosophy of\\nmathematics: selected readings (2nd ed.)  \\n[*1970]  \\n“Ontological proof”, manuscript.\\nPrinted in Gödel 1995, pp. 403–404.  \\n[*1970a]  \\n“Some considerations leading to the probable\\nconclusion that the true power of the continuum is\\nℵ ”, manuscript. Printed in Gödel 1995,\\npp. 420–422.  \\n2  \\n[*1970b]  \\n“A proof of Cantor’s continuum\\nhypothesis from a highly plausible axioms about orders of\\ngrowth”, manuscript. Printed in Gödel 1995, pp.\\n422–423.  \\nSecondary Sources  \\nAvigad, J. and S. Feferman, 1998, “Gödel’s\\nFunctional (‘Dialectica’) Interpretation”, in (Studies in Logic and the\\nFoundations of Mathematics, Volume 137), Samuel Buss (ed.), Amsterdam:\\nNorth-Holland, pp. 337-405.  \\nHandbook of Proof Theory  \\nAwodey, S. and A. W. Carus, 2010, “Gödel and\\nCarnap”, in ,\\nSolomon Feferman, Charles Parsons & Stephen G. Simpson (eds.),\\nCambridge: Cambridge University Press.  \\nKurt Gödel: Essays for his Centennial  \\nBaaz, M., and C. Papadimitriou, D.Scott, H. Putnam, and C. Harper\\n(eds.), 2011, , Cambridge: Cambridge University Press.  \\nKurt Gödel and the Foundations of Mathematics:\\nHorizons of Truth  \\nBadesa, C., and P. Mancosu, and R. Zach, 2009, “The\\nDevelopment of Mathematical Logic from Russell to Tarski,\\n1900–1935”, in Leila Haaparanta (ed.), . New York and Oxford: Oxford University\\nPress:318–470..  \\nThe History of\\nModern Logic  \\nBarwise, Jon (ed.), 1977, (Studies in Logic and the Foundations of Mathematics, Volume 90),\\nAmsterdam: North-Holland Publishing Co.  \\nHandbook of Mathematical Logic  \\nBehmann, Heinrich, 1922, “Beiträge, Algebra, Logik,\\ninsbesodere zum Entscheidungsproblem”, , 86: 419–432.  \\nMathematische\\nAnnalen  \\nBenacerraf, P. and H. Putnam (eds.), 1983, , Cambridge: Cambridge University\\nPress, 2nd edition.  \\nPhilosophy of\\nMathematics: Selected Readings  \\nBernays, Paul, 1926, “Axiomatische Untersuchung des\\nAussagen-Kalkuls der ‘Principia Mathematica’”, , 25(1): 305–320.  \\nMathematisches Zeitschrift  \\nBezboruah, A., and J.C. Sheperdson, 1976,\\n“Gödel’s second incompleteness theorem for\\n\\\\(Q\\\\)”, , 41 (2):\\n503–512.  \\nJournal of Symbolic Logic  \\nBolzano, Bernard, 1969, , Sections\\n349–391, in ,\\nReihe I/Band 13, edited and with an introduction by Jan Berg,\\nStuttgart-Bad Cannstatt: Frommann Holzboog.  \\nWissenschaftslehre  \\nBernard Bolzano — Gesamtausgabe  \\nBurgess, John, 2009, “”Intuitions of Three Kinds in\\nGödel’s Views on the Continuum“”, in Kennedy, J. (ed.) Cambridge: Cambridge\\nUniversity Press, 2014.  \\nInterpreting Gödel  \\nBuss, Samuel R., 1994, “On Gödel’s Theorems on\\nLengths of Proofs. I. Number of Lines and Speedup for\\nArithmetics”, , 59(3):\\n737–756.  \\nJournal of Symbolic Logic  \\nChisholm, R., 1948, “The Problem of Empiricism”, , 45: 512–7.  \\nThe Journal of Philosophy  \\nCohen, Paul, 1963, “The Independence of the Continuum\\nHypothesis”, , 50: 1143–1148.  \\nProceedings of the National Academy of Sciences\\nof the U.S.A.  \\nCrocco, G., 2003, “Gödel, Carnap and the Fregean\\nHeritage”, , 27:\\n171–191.  \\nHistory and Philosophy of Logic  \\n–––, 2006, “Gödel on Concepts”, , 137(1,2): 21–41.  \\nSynthese  \\nDawson, Jr., John W., 1997, , Wellesley, MA: A. K. Peters, Ltd.  \\nLogical dilemmas: The Life and\\nWork of Kurt Gödel  \\nDehornoy, Patrick, 2004, “Progrès récents sur\\nl’hypothèse du continu (d’après\\nWoodin)”, , 294: viii,\\n147–172.  \\nAstérisque  \\nDetlefsen, Michael, 1986, , Dordrecht: D. Reidel.  \\nHilbert’s Program: An essay on\\nmathematical instrumentalism  \\n–––, 2001, “What Does Gödel’s\\nSecond theorem Say?”, , 9(1):\\n37–71.  \\nPhilosophia Mathemathica  \\n–––, 2014, “Completeness and the Ends of\\nAxiomatization”, in , Kennedy,\\nJ. (ed.) Cambridge: Cambridge University Press, 2014.  \\nInterpreting Gödel  \\nDreben, B. and J. van Heijenoort, 1986, “Introductory Note\\nto 1929, 1930 and 1930a”, in Gödel 1986, pp.\\n44–59.  \\nEdwards, Paul (ed.), 1967, , New York: MacMillan.  \\nThe Encyclopedia of\\nPhilosophy  \\nEhrenfeucht, A. and J. Mycielski, 1971, “Abbreviating Proofs\\nby Adding New Axioms”, , 77: 366–367.  \\nBulletin of the American Mathematical\\nSociety  \\nFeferman, Solomon, 1960/1961, “Arithmetization of\\nMetamathematics in a General Setting”, , 49: 35–92.  \\nFundamenta\\nMathematicae  \\n–––, 1993, “Gödel’s Dialectica\\nInterpretation and Its Two-way Stretch”, in (Lecture Notes in Computer Science, Volume\\n713), G. Gottlob, A. Leitsch, and D. Mundici (eds.), Berlin: Springer,\\npp. 23–40.  \\nComputational\\nLogic and Proof Theory  \\n–––, 1986, “Gödel’s Life and\\nWork”, in Gödel 1986, pp. 1–34.  \\n–––, 1988, “Hilbert’s Program\\nRelativized: Proof-Theoretical and Foundational Reductions”, , 53: 364–384.  \\nJournal of Symbolic Logic  \\n–––, 1996, “Proof Theory”, in , D. Borchrt (ed.),\\nNew York: MacMillan, pp. 466–469.  \\nThe Encyclopedia of Philosophy Supplement  \\nFeferman, S., and H. Friedman, P. Maddy, and J. Steel, 2000,\\n“Does Mathematics Need New Axioms?”, , 6(4): 401–446.  \\nBulletin of\\nSymbolic Logic  \\nFeferman, S., C. Parsons, and S. Simpson (eds.), 2010, (Lecture Notes in Logic,\\n33), Cambridge: Cambridge University Press.  \\nKurt\\nGödel: Essays for his Centennial  \\nFeigl, H. and A. Blumberg, 1931, “Logical Positivism. A New\\nMovement in European Philosophy”, , 28: 281–296.  \\nJournal of\\nPhilosophy  \\nFloyd, J. and A. Kanamori, 2006, “How Gödel Transformed\\nSet Theory”, , 53(4): 419–427.  \\nNotices of the American Mathematical\\nSociety  \\nFolina, Janet, 2014, “Gödel on How to Have your\\nMathematics and Know it Too”, in , Kennedy, J. (ed.) Cambridge: Cambridge University\\nPress, 2014.  \\nInterpreting\\nGödel  \\nFøllesdal, Dagfinn, 1995, “Gödel and\\nHusserl”, in (Synthese\\nLibrary, Volume 251), J. Hintikka (ed.), Dordrecht, Boston: Kluwer,\\npp. 427–446.  \\nFrom Dedekind to Gödel  \\nForeman, Matthew, 1998, “Generic Large Cardinals: New Axioms\\nfor Mathematics?”, in , Extra\\nVolume, Proceedings of the International Congress of Mathematicians,\\nII, pp. 11–21\\n [ (in compressed Postscript)].  \\nDocumenta Mathematica  \\navailable online  \\nFranks, Curtis, 2009, “The Autonomy of Mathematical\\nKnowledge: Hilbert’s Program Revisited”, Cambridge:\\nCambridge University Press.  \\n–––, 2011, “Stanley Tennenbaum’s\\nSocrates”, in , Kennedy, J. and Kossak, R.,\\n(eds.), Lecture Notes in Logic, 36, Cambridge: Cambridge University\\nPress, 2011.  \\nSet Theory, Arithmetic and Foundations of\\nMathematics: Theorems, Philosophies  \\n–––, 2014, “Logical Completeness, Form and\\nContent: An Archaeology”, in ,\\nKennedy, J. (ed.) Cambridge: Cambridge University Press, 2014.  \\nInterpreting Gödel  \\nGaifman, H., 2000, “What Godel’s Incompleteness Result\\nDoes and Does Not Show”, , 97 (8):\\n462–471.  \\nJournal of Philosophy  \\nGarson, James, 2003, “Modal Logic”, in , Fall 2003 Edition, Edward N.\\nZalta (ed.), URL =\\n < >.  \\nThe\\nStanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/fall2003/entries/logic-modal/  \\nGlivenko, V., 1929, “Sur quelques points de la logique de m.\\nBrouwer.”, , 15: 183–188.  \\nAcadémie Royale de Belgique, Bulletin de\\nla Classe des Sciences  \\nGottwald, Siegfried, 2004, “Many-valued Logic”, in , Winter 2004 Edition,\\nEdward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/win2004/entries/logic-manyvalued/  \\nGödel, Rudolf, 1983, “History of the Gödel\\nFamily”, Susan Simonsin (trans.), in Weingartner and Schmetterer\\n1987, pp. 11–27.  \\nHauser, Kai, 2006, “Gödel’s Program Revisited,\\nPart 1: the Turn to Phenomenology”, , 12 (4): 529–590.  \\nBulletin of Symbolic\\nLogic  \\nHeyting, Arendt, 1930, “Die formalen Regeln der\\nintuitionistischen Logik”, ,\\nII, pp. 42–56.  \\nSitzungsberichte der Preussischen\\nAkademie der Wissenschaften, physikalisch-mathematische Klasse  \\nHilbert, David, 1926, “Über das Unendliche”, , 95: 161–190.  \\nMathematische Annalen  \\nHilbert, D. and W. Ackermann, 1928, , Berlin: Springer-Verlag.  \\nGrundzüge der\\ntheoretischen Logik  \\nHilbert, D. and P. Bernays, 1934, , Volume 1, Berlin: Springer-Verlag.  \\nGrundlagen der\\nMathematik  \\n–––, 1939, ,\\nVolume II, Berlin: Springer-Verlag.  \\nGrundlagen der Mathematik  \\nHodges, Wilfrid, 2005, “Model Theory”, in , Fall 2005 Edition, Edward N.\\nZalta (ed.), URL =\\n < >.  \\nThe\\nStanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/fall2005/entries/model-theory/  \\nHusserl, Edmund, 1911, “Philosophie als strenge\\nWissenschaft”, , 1: 289–341.  \\nLogos  \\nJaśkowski, Stanisław, 1936, “Investigations into\\nthe System of Intuitionist Logic”, , 34(2)\\n(1975): 117–120. (Translated by S. McCall from the French\\n“Rechereches sur le système de la logique\\nintuitioniste” in , Volume VI, Hermann, Paris, 1936, pp.\\n58–61.)  \\nStudia Logica  \\nActes du Congrés International de\\nPhilosophie Scientifique  \\nJech, Thomas, 2003, , (Springer Monographs in\\nMathematics), Berlin: Springer-Verlag. 3rd millennium edition, revised\\nand expanded.  \\nSet theory  \\nJensen, R. Björn, 1972, “The Fine Structure of the\\nConstructible Hierarchy” (with a section by Jack Silver), , 4: 229–308; Erratum, 4\\n(1972): 443.  \\nAnnals of Mathematical Logic  \\nKanamori, Aki, 1996, “The Mathematical Development of Set\\ntheory from Cantor to Cohen.” , 2(1): 1–71.  \\nBulletin of Symbolic\\nLogic  \\n–––, 2006, “Levy and Set Theory”, , 140(3): 233–252.  \\nAnnals of Pure and Applied Logic  \\nKennedy, Juliette, 2006, “Incompleteness — A Book\\nReview,” ,\\n53(4): 448–455.  \\nNotices of the American Mathematical Societ  \\n–––, 2011, “Gödel’s Thesis: An\\nAppreciation” in , M. Baaz, C. Papadimitriou, D.\\nScott, H. Putnam, and C. Harper (eds.), Cambridge: Cambridge\\nUniversity Press 95–110.  \\nKurt Gödel and the Foundations of\\nMathematics: Horizons of Truth  \\n–––, 2013, “On Formalism Freeness:\\nImplementing Gödel’s 1946 Princeton Bicentennial\\nLecture”, , 19(3):\\n351–393.  \\nBulletin of Symbolic Logic  \\n–––, 2014, “Gödel’s 1946\\nPrinceton Bicentennial Lecture: An Appreciation”, in , Cambridge:\\nCambridge University Press.  \\nInterpreting Gödel: Critical Essays  \\nKennedy, Juliette (ed.), 2014, , Cambridge: Cambridge University Press.  \\nInterpreting Gödel:\\nCritical Essays  \\nKennedy, J. and van Atten, M., 2003, “On the Philosophical\\nDevelopment of Kurt Gödel”, , 9(4): 425–476. Reprinted in , Solomon Feferman, Charles Parsons and\\nStephen G. Simpson (eds.), Cambridge: Cambridge University Press.  \\nBulletin of Symbolic\\nLogic  \\nKurt Gödel:\\nEssays for his Centennial  \\n–––, 2004, “Gödel’s Modernism:\\nOn Set-theoretic Incompleteness”, , 25(2): 289–349. (See the Erratum in , 26(1) (2005), page\\nfacing contents.)  \\nGraduate Faculty\\nPhilosophy Journal  \\nGraduate Faculty Philosophy Journal  \\n–––, 2009, “Gödel’s Modernism:\\nOn Set-theoretic Incompleteness, Revisited”, in , S.\\nLinström, E. Palmgren, K. Segerberg, and V. Stoltenberg-Hansen\\n(eds.), Berlin: Springer: 303–356.  \\nLogicism,\\nIntuitionism and Formalism: What has become of them?  \\n–––, 2009, “Gödel’s\\nLogic”, in D. Gabbay and J. Woods (eds.), , Volume 5,\\nAmsterdam: Elsevier: 449–509.  \\nThe Handbook of\\nthe History of Logic: Logic from Russell to Gödel  \\nKleene, S. C., 1987, “Gödel’s Impression on\\nStudents of Logic in the 1930s”, in Weingartner and Schmetterer\\n1987, pp. 49–64.  \\nKoellner, Peter, 2014, “Large Cardinals and\\nDeterminacy”, (Spring Edition), Edward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/spr2014/entries/large-cardinals-determinacy/  \\nKreisel, Georg, 1980, “Kurt Gödel, 28 April 1906\\n– 14 January 1978”, , 26: 148–224. Corrigenda, 27 (1981): 697;\\nfurther corrigenda, 28 (1982): 697.  \\nBiographical Memoirs of Fellows of\\nthe Royal Society  \\n–––, 1988, “Review of Kurt Gödel: , Volume I”, , 29(1): 160–181.  \\nCollected works  \\nNotre Dame Journal of\\nFormal Logic  \\n–––, 1990, “Review of Kurt Gödel: , Volume II”, , 31(4): 602–641.  \\nCollected Works  \\nNotre Dame Journal of\\nFormal Logic  \\n–––, 1998, “Second Thoughts Around Some of\\nGödel’s Writings: A Non-academic Option”, , 114(1): 99–160.  \\nSynthese  \\nKripke, Saul, 2009, “The collapse of the Hilbert program:\\nwhy a system cannot prove its own 1-consistency”, , 15 (2): 229–231.  \\nBulletin\\nof Symbolic Logic  \\nKunen, Kenneth, 1983, , (Studies in Logic and the Foundations of\\nMathematics, Volume 102), Amsterdam: North-Holland Publishing Co.\\nReprint of the 1980 original.  \\nSet Theory: An Introduction to\\nIndependence Proofs  \\nLöb, M. H., 1956, “Formal Systems of Constructive\\nMathematics”, , 21:\\n63–75.  \\nJournal of Symbolic Logic  \\nLöwenheim, L., 1915, “Über Möglichkeiten im\\nRelativkalkül”, , 76(4):\\n447–470.  \\nMathematische Annalen  \\nŁukasiewicz, Jan, 1970, , (Studies in\\nLogic and the Foundations of Mathematics), L. Borkowski (ed.),\\nAmsterdam: North-Holland Publishing Co.  \\nSelected works  \\nMaddy, Penelope, 1990, , New York:\\nClarendon Press.  \\nRealism in Mathematics  \\nMaddy, Penelope, 2011, , Oxford:\\nOxford University Press.  \\nDefending the Axioms  \\nMal’cev, Anatoli Ivanovic, 1971, (Studies in\\nLogic and the Foundations of Mathematics, Volume 66), translated,\\nedited, and provided with supplementary notes by Benjamin Franklin\\nWells, III, Amsterdam: North-Holland Publishing Co.  \\nThe Metamathematics of\\nAlgebraic Systems. Collected Papers: 1936–1967  \\nMancosu, Paolo, 1998, , Oxford: Oxford\\nUniversity Press.  \\nFrom Brouwer to Hilbert. The Debate on\\nthe Foundations of Mathematics in the 1920s  \\n–––, 2004, “Review of Kurt Gödel, , Volumes IV and V”, , 45: 109–125.  \\nCollected Works  \\nNotre Dame\\nJournal of Formal Logic  \\nMartin, D.A., 2005, “Gödel’s Conceptual\\nRealism”, , 11:\\n207–224.  \\nBulletin of Symbolic Logic  \\nMcKinsey, J. C. C. and A. Tarski, 1948, “Some Theorems About\\nthe Sentential Calculi of Lewis and Heyting”, , 13: 1–15.  \\nJournal of\\nSymbolic Logic  \\nMostowski, Andrzej, 1949, “An Undecidable Arithmetical\\nStatement”, , 36:\\n143–164.  \\nFundamenta Mathematicae  \\n–––, 1982, , Westport, CT: Greenwood Press. Reprint of the 1952\\noriginal.  \\nSentences Undecidable in\\nFormalized Arithmetic: An Exposition of the Theory of Kurt\\nGödel  \\nOliva, Paulo, 2006, “Unifying Functional\\nInterpretations”, ,\\n47(2): 263–290.  \\nNotre Dame Journal of Formal Logic  \\nParikh, Rohit, 1971, “Existence and Feasibility in\\nArithmetic”, , 36:\\n494–508.  \\nJournal of Symbolic Logic  \\nParsons, Charles, 1995a, “Platonism and Mathematical\\nIntuition in Kurt Gödel’s Thought”, , 1(1): 44–74.  \\nBulletin of\\nSymbolic Logic  \\n–––, 1995b, “Quine and Gödel on\\nAnalyticity”, in , Cambridge:\\nCambridge University Press, pp. 297–313.  \\nOn Quine: New Essays  \\n–––, 2000, “Reason and Intuition”, , 125(3): 299–315.  \\nSynthese  \\n–––, 2002, “Realism and the Debate on\\nImpredicativity, 1917–1944”, in ,\\n(Lecture Notes in Logic, Volume 15), W. Sieg, R. Sommer, and C.\\nTalcott (eds.), Urbana, IL: Association of Symbolic Logic, pp.\\n372–389.  \\nReflections on the\\nFoundations of Mathematics: Essays in Honor of Solomon Feferman  \\n–––, 2010, “Gödel and Philosophical\\nIdealism” , 18 (2):\\n166–192.  \\nPhilosophia Mathematica  \\n–––, 2014, “Analyticity for\\nRealists”, in , Kennedy, J.\\n(ed.) Cambridge: Cambridge University Press, 2014.  \\nInterpreting Gödel  \\nPoonen, Bjorn, 2014, “Undecidable Problems: A\\nSampler”, in ,\\nCambridge: Cambridge University Press.  \\nInterpreting Gödel: Critical Essays  \\nPost, Emil L., 1921, “Introduction to a General Theory of\\nElementary Propositions”, , 43(3): 163–185.  \\nAmerican Journal of\\nMathematics  \\nPudlák, Pavel, 1996, “On the lengths of proofs of\\nconsistency: a survey of results”, , 2: 65-86.  \\nAnnals of the Kurt\\nGödel Society  \\nRaatikainen, P., 2005, “On the Philosophical Relevance of\\nGödel’s Incompleteness Theorems”, , 59 (4): 513–534.  \\nRevue\\nInternationale de Philosophie  \\nRogers, Jr., Hartley, 1967, , New York: McGraw-Hill Book Co.  \\nTheory of Recursive Functions and\\nEffective Computability  \\nRosser, J.B., 1936, “Extensions of Some Theorems of\\nGödel and Church”, ,\\n1(3): 87–91.  \\nJournal of Symbolic Logic  \\nScott, Dana, 1961, “Measurable Cardinals and Constructible\\nSets”, (Série des Science, Mathématiques,\\nAstronomiques et Physiques), 9: 521–524.  \\nBulleint de l’Academie Polonaise des\\nSciences  \\nShelah, Saharon, 2014, “Reflecting on Logical Dreams”,\\nin , Cambridge:\\nCambridge University Press.  \\nInterpreting Gödel: Critical Essays  \\nSieg, Wilfried, 1988, “Hilbert’s Program Sixty Years\\nLater”, , 53(2):\\n338–348.  \\nJournal of Symbolic Logic  \\n–––, 1990, “Relative Consistency and\\nAccessible Domains”, , 84(2):\\n259–297.  \\nSynthese  \\n–––, 1999, “Hilbert’s Programs:\\n1917–1922”, , 5(1):\\n1–44.  \\nBulletin of Symbolic Logic  \\n–––, 2006, “Gödel on\\nComputability”, , 14:\\n189–207.  \\nPhilosophia Mathematica  \\nSierpinski, Wacław, 1947, “L’hypothèse\\ngénéralisée du continu et l’axiome du\\nchoix”, , 34: 1–5.  \\nFundamenta Mathematicae  \\nSigmund, Karl, 2006, “Pictures at an Exhibition”, , 53(4):\\n428–432.  \\nNotices of the American Mathematical Society  \\nSkolem, Thoralf, 1920, “Logisch-kombinatorische\\nUntersuchungen über die Erfüllbarkeit oder Beweisbarkeit\\nmathematischer Sätze nebst einem Theoreme über dichte\\nMengen”, , I. ,\\nNumber 4, pp. 1–36. Reprinted in Skolem 1970, pp.\\n103–136.  \\nSkrifter utgit av Videnskappsselskapet i\\nKristiania  \\nMatematisk-naturvidenskabelig klasse  \\n–––, 1923, “Einige Bemerkungen zur\\naxiomatischen Begründung der Mengenlehre”, ,\\nHelsinki, pp. 217–232. Reprinted in Skolem 1970, pp.\\n137–152.  \\nMatematikerkongressen i Helsingfors den 4–7 Juli 1922, Den\\nfemte skandinaviska matematikerkongressen, Redogörelse  \\n–––, 1933, “Über die\\nUnmöglichkeit einer vollständigen Charakterisierung der\\nZahlenreihe mittels eines endlichen Axiomensystems”, , 10: 73–82.  \\nNorsk\\nMatematisk forenings skrifter  \\n–––, 1970, ,\\nJens Erik Fenstad (ed.), Oslo: Universitetsforlaget.  \\nSelected Works in Logic  \\nSmith, David Woodruff, 2005, “Phenomenology”, in (Winter Edition),\\nEdward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/win2005/entries/phenomenology/  \\nSolovay, Robert, 1990, “Introductory Note to 1938, 1939,\\n1939a, 1940”, in Gödel 1990, pp. 1–25.  \\nSteel, John, 2000, “Mathematics Needs New Axioms”, , 6(4): 422–433.  \\nBulletin of Symbolic Logic  \\nSteel, John, 2014, “Gödel’s Program”, in , Kennedy, J. (ed.) Cambridge:\\nCambridge University Press, 2014.  \\nInterpreting Gödel  \\nTait, William, 1967, “Intensional Interpretations of\\nFunctionals of Finite Type I,” , 32(2): 198–212.  \\nJournal of Symbolic\\nLogic  \\n–––, 1981, “Finitism”, , 78: 524–556. Reprinted in Tait 2005, pp.\\n21–42.  \\nJournal\\nof Philosophy  \\n–––, 1986, “Truth and Proof: The Platonism\\nof Mathematics”, , 69(3): 341–370.\\nReprinted in Tait 2005, pp. 61–88.  \\nSynthese  \\n–––, 2001, “Gödel’s Unpublished\\nPapers on Foundations of Mathematics”, , 9(1): 87–126. Reprinted in Tait 2005, pp.\\n276–313.  \\nPhilosophia\\nMathematica  \\n–––, 2002, “Remarks on Finitism”, in (Lecture Notes in Logic, Volume 15), W. Sieg, R.\\nSommer, and C. Talcott (eds.), Urbana, IL: Association of Symbolic\\nLogic, pp. 410–419. Reprinted in Tait 2005, pp.\\n43–53.  \\nReflections on the Foundations of Mathematics: Essays in Honor of\\nSolomon Feferman  \\n–––, 2005, (Logic\\nand Computation in Philosophy), New York: Oxford University\\nPress.  \\nThe Provenance of Pure Reason:\\nEssays in the Philosophy of Mathematics and its History  \\n–––, 2006, “Gödel’s\\ncorrespondence on proof theory and constructive\\nmathematics” , 14 (1):\\n76–111.  \\nPhilosophia Mathematica  \\n–––, 2006, “Gödel’s\\ninterpretation of intuitionism”, , 14 (2): 208–228.  \\nPhilosophia\\nMathematica  \\nTaussky-Todd, Olga, 1983, “Remembrances of Kurt\\nGödel”, in Weingartner and Schmetterer 1987, pp.\\n29–41.  \\nTieszen, Richard, 1992, “Kurt Gödel and\\nPhenomenology”, , 59(2):\\n176–194.  \\nPhilosophy of Science  \\n–––, 2002, “Gödel and the Intuition\\nof Concepts”, , 133 (3): 363–391.  \\nSynthese  \\n–––, 2005, , Cambridge: Cambridge University\\nPress.  \\nPhenomenology, Logic and the\\nPhilosophy of Mathematics  \\n–––, 2011, , Oxford: Oxford University\\nPress.  \\nAfter Gödel: Platonism and\\nRationalism in Mathematics and Logic  \\nToledo, Sue, 2011, “Sue Toledo’s Notes of her\\nConversations with Kurt Gödel in 1972-5”, in (Lecture Notes in Logic, 36), Kennedy, J. and\\nKossak, R., (eds.), Cambridge: Cambridge University Press,\\nforthcoming.  \\nSet\\nTheory, Arithmetic and Foundations of Mathematics: Theorems,\\nPhilosophies  \\nTragesser, Robert, 1977, , Ithaca:\\nCornell University Press.  \\nPhenomenology and Logic  \\n–––, 1984, , (Series: Modern European Philosophy), Cambridge:\\nCambridge University Press.  \\nHusserl and Realism in Logic and\\nMathematics  \\n–––, 1989, “Sense Perceptual Intuition,\\nMathematical Existence, and Logical Imagination”, , 4(2): 154–194.  \\nPhilosphia\\nMathematica  \\nTroelstra, A. S., 1986, “Note to and ”, in Gödel 1990, pp. 217–241.  \\n1958  \\n1972  \\nTroelstra, A. S. (ed.), 1973, , (Lecture Notes in\\nMathematics, Volume 344), Berlin: Springer-Verlag.  \\nMetamathematical Investigation\\nof Intuitionistic Arithmetic and Analysis  \\nTuring, A. M., 1937, “On Computable Numbers, with an\\nApplication to the Entscheidungsproblem”, (Series 2), 42: 230–265.  \\nProceedings of the\\nLondon Mathematical Society  \\nvan Atten, Mark, 2005, “On Gödel’s Awareness of\\nSkolem’s Helsinki Lecture”, , 26(4): 321–326.  \\nHistory and Philosophy of\\nLogic  \\n–––, 2006, “Two Draft Letters from\\nGödel on Self-knowledge of Reason”, , 14(2): 255–261.  \\nPhilosophia\\nMathematica  \\n–––, 2015, “Essays on Gödel’s\\nReception of Leibniz, Husserl and Brouwer”, Springer.  \\nvan Heijenoort, J. (ed.), 1967, , Cambridge, MA:\\nHarvard University Press.  \\nFrom Frege to Gödel: A\\nsourcebook in mathematical logic, 1879–1931  \\nvan Oosten, Jaap, 2008, (Studies in Logic and Foundations of\\nMathematics: Volume 152), Amsterdam: Elsevier.  \\nRealizability: An Introduction to its\\nCategorical Side  \\nvon Neumann, John, 2005, (History of Mathematics, Volume 27), foreword by P. Lax,\\nintroduction by Marina von Neumann Whitman, preface and introductory\\ncomments by Miklós Rédei (ed.), Providence, RI: American\\nMathematical Society.  \\nJohn von Neumann: Selected\\nLetters  \\nVäänänen, Jouko, 2014, “Multiverse Set Theory\\nand Absolutely Undecidable Propositions”, in , J. Kennedy (ed.), Cambridge: Cambridge University\\nPress, 2014.  \\nInterpreting\\nGödel  \\nWang, Hao, 1957, “The Axiomatization of Arithmetic”, , 22: 145–158.  \\nJournal of Symbolic Logic  \\n–––, 1973, , London: Routledge.  \\nFrom Mathematics to\\nPhilosophy  \\n–––, 1981, “Some Facts about Kurt\\nGödel”, , 46(3):\\n653–659.  \\nJournal of Symbolic Logic  \\n–––, 1987, , Cambridge, MA: MIT Press.  \\nReflections on Kurt\\nGödel  \\n–––, 1993, , New York: Dover Publications Inc., 2nd edition.  \\nPopular Lectures on Mathematical\\nLogic  \\n–––, 1996, (Representation and Mind), Cambridge,\\nMA: MIT Press.  \\nA Logical Lourney: From\\nGödel to Philosophy  \\nWeingartner, P., and L. Schmetterer (eds.), 1987, , (History of Logic,\\nNumber 4), Naples: Bibliopolis.  \\nGödel\\nRemembered: Salzburg 10–12 July 1983  \\nWilkie, Alex, and J.B. Paris, 1987, “On the scheme of\\ninduction for bounded arithmetic formulas”, 35:\\n261–302.  \\nWoodin, W. Hugh, 1988, “Supercompact Cardinals, Sets of\\nReals, and Weakly Homogeneous Trees”, , 85(18):\\n6587–6591.  \\nProceedings of the\\nNational Academy of Sciences of the U.S.A.  \\n–––, 2001a, “The Continuum Hypothesis.\\nI”, ,\\n48(6): 567–576.  \\nNotices of the American Mathematical Society  \\n–––, 2001b, “The Continuum Hypothesis.\\nII”, ,\\n48(7): 681–690.  \\nNotices of the American Mathematical Society  \\n–––, 2002, “Correction: ‘The\\nContinuum Hypothesis. II’”, , 49(1): 46.  \\nNotices of the American\\nMathematical Society  \\nYourgrau, Palle, 2005, , New York: Basic Books.  \\nA World Without Time. The Forgotten\\nLegacy of Gödel and Einstein  \\nZach, Richard, 1999, “Completeness Before Post: Bernays,\\nHilbert, and the Development of Propositional Logic”, , 5(3): 331–366.  \\nBulletin of Symbolic Logic  \\n–––, 2003, “Hilbert’s\\nProgram”, in (Fall Edition), Edward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/fall2003/entries/hilbert-program/'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': 'Academic Tools'}, page_content='Academic Tools'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': 'Academic Tools'}, page_content='.  \\nHow to cite this entry  \\nat the .  \\nPreview the PDF version of this entry  \\nFriends of the SEP Society  \\nat the Internet Philosophy Ontology Project (InPhO).  \\nLook up topics and thinkers related to this entry  \\nat , with links to its database.  \\nEnhanced bibliography for this entry  \\nPhilPapers  \\nOther Internet Resources  \\nAvigad, Jeremy,\\n “ ”,\\n manuscript in PDF available online.  \\nGödel and the metamathematical tradition  \\nKoellner, Peter,\\n “ ”,\\n manuscript in PDF available online.  \\nTruth in Mathematics:The question of Pluralism  \\n.  \\nThe Bernays Project  \\nRelated Entries  \\n| | | | | | | | | | | |  \\nGödel, Kurt: incompleteness theorems  \\nHilbert, David: program in the foundations of mathematics  \\nHusserl, Edmund  \\nLeibniz, Gottfried Wilhelm  \\nmathematics, philosophy of: intuitionism  \\nmathematics, philosophy of: Platonism  \\nmodel theory  \\nmodel theory: first-order  \\nphenomenology  \\nrealism  \\nset theory  \\nset theory: continuum hypothesis  \\nset theory: large cardinals and determinacy'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': 'Academic Tools', 'Header 3': 'Acknowledgments'}, page_content='Acknowledgments'),\n",
       " Document(metadata={}, page_content='This entry was very much improved by discussion and correspondence\\nwith the following: Aki Kanamori, who made helpful corrections and\\ncomments to section 2.4; Jouko Väänänen, whose\\nexpertise in all areas of mathematical logic the author benefited from\\nin a great many invaluable discussions regarding the material in\\nsection 2; my sub-editor Richard Zach, whose many important and\\nhelpful suggestions led to a vast improvement of this entry, and an\\nanonymous referee for helpful comments and corrections. The author is\\ngrateful to the NWO for their support during the last period of the\\nwriting of this entry, to the Institute for Advanced Study for their\\nhospitality during the writing of this entry, and to Marcia Tucker of\\nthe IAS and the Rare Books and Special Collections department of\\nFirestone Library for all of their assistance over the years .  \\nby < >  \\nCopyright © 2015  \\nJuliette Kennedy  \\njuliette kennedy helsinki fi  \\n.  \\n@  \\n.  \\nOpen access to the SEP is made possible by a world-wide funding initiative. The Encyclopedia Now Needs Your Support Please Read How You Can Help Keep the Encyclopedia Free  \\nEnd footer menu End mirrors End site credits'),\n",
       " Document(metadata={'Header 4': 'Browse'}, page_content='Browse'),\n",
       " Document(metadata={'Header 4': 'Browse'}, page_content=\"Table of Contents  \\nWhat's New  \\nRandom Entry  \\nChronological  \\nArchives\"),\n",
       " Document(metadata={'Header 4': 'About'}, page_content='About'),\n",
       " Document(metadata={'Header 4': 'About'}, page_content='Editorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nAccessibility  \\nContact'),\n",
       " Document(metadata={'Header 4': 'Support SEP'}, page_content='Support SEP'),\n",
       " Document(metadata={'Header 4': 'Support SEP'}, page_content='Support the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries'),\n",
       " Document(metadata={'Header 4': 'Mirror Sites'}, page_content='Mirror Sites'),\n",
       " Document(metadata={}, page_content=\"View this site from another server:  \\nUSA (Main Site)  \\nPhilosophy, Stanford University  \\nInfo about mirror sites  \\nThe Stanford Encyclopedia of Philosophy is by , Department of Philosophy, Stanford University  \\ncopyright © 2023  \\nThe Metaphysics Research Lab  \\nLibrary of Congress Catalog Data: ISSN 1095-5054  \\n$('.dropdown-toggle').dropdown();\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c047822",
   "metadata": {},
   "outputs": [],
   "source": [
    "## JSON splitter\n",
    "import json\n",
    "import requests\n",
    "\n",
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ea2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "\n",
    "json_splitter = RecursiveJsonSplitter(max_chunk_size=300)\n",
    "json_chunks = json_splitter.split_json(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9712351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}, 'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'tags': ['tracer-sessions'], 'summary': 'Get Tracing Project Prebuilt Dashboard', 'description': 'Get a prebuilt dashboard for a tracing project.'}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'operationId': 'get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post', 'security': [{'API Key': []}, {'Tenant ID': []}, {'Bearer Auth': []}]}}}}\n",
      "{'paths': {'/api/v1/sessions/{session_id}/dashboard': {'post': {'parameters': [{'name': 'session_id', 'in': 'path', 'required': True, 'schema': {'type': 'string', 'format': 'uuid', 'title': 'Session Id'}}, {'name': 'accept', 'in': 'header', 'required': False, 'schema': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'title': 'Accept'}}]}}}}\n"
     ]
    }
   ],
   "source": [
    "_ = [print(chunk) for chunk in json_chunks[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2333561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Get Tracing Project Prebuilt Dashboard\", \"description\": \"Get a prebuilt dashboard for a tracing project.\"}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"operationId\": \"get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}'\n",
      "page_content='{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}'\n"
     ]
    }
   ],
   "source": [
    "docs = json_splitter.create_documents(texts=[json_data])\n",
    "_ = [print(doc) for doc in docs[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d97ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}, \"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"tags\": [\"tracer-sessions\"], \"summary\": \"Get Tracing Project Prebuilt Dashboard\", \"description\": \"Get a prebuilt dashboard for a tracing project.\"}}}}\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"operationId\": \"get_tracing_project_prebuilt_dashboard_api_v1_sessions__session_id__dashboard_post\", \"security\": [{\"API Key\": []}, {\"Tenant ID\": []}, {\"Bearer Auth\": []}]}}}}\n",
      "{\"paths\": {\"/api/v1/sessions/{session_id}/dashboard\": {\"post\": {\"parameters\": [{\"name\": \"session_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Session Id\"}}, {\"name\": \"accept\", \"in\": \"header\", \"required\": false, \"schema\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"null\"}], \"title\": \"Accept\"}}]}}}}\n"
     ]
    }
   ],
   "source": [
    "texts = json_splitter.split_text(json_data)\n",
    "_ = [print(text) for text in texts[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5587a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"data/attention.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c55bb371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.'),\n",
       " Document(metadata={}, page_content='== History =='),\n",
       " Document(metadata={}, page_content=\"The use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI's release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI's warnings against sexual imagery, SD's public release led to dedicated\"),\n",
       " Document(metadata={}, page_content=\"imagery, SD's public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\"),\n",
       " Document(metadata={}, page_content='=== AI-generated influencers ==='),\n",
       " Document(metadata={}, page_content='One application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.'),\n",
       " Document(metadata={}, page_content='=== The growth of AI porn sites ==='),\n",
       " Document(metadata={}, page_content='By 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content.'),\n",
       " Document(metadata={}, page_content='the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.'),\n",
       " Document(metadata={}, page_content='== Ethical concerns and misuse =='),\n",
       " Document(metadata={}, page_content='The growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.'),\n",
       " Document(metadata={}, page_content='OpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.'),\n",
       " Document(metadata={}, page_content='=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Francisco filed a landmark lawsuit to shut down \"undress\" apps that allow users to generate non-consensual AI nude images, citing violations of state laws. The case aligns with California\\'s recent legislation—SB 926, SB 942, and SB 981—championed by Senators Aisha Wahab a'),\n",
       " Document(metadata={}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.'),\n",
       " Document(metadata={}, page_content='Generative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI,'),\n",
       " Document(metadata={}, page_content='generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.'),\n",
       " Document(metadata={}, page_content='Generative AI has raised many ethical questions as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.'),\n",
       " Document(metadata={}, page_content='Generative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.'),\n",
       " Document(metadata={}, page_content='== History =='),\n",
       " Document(metadata={}, page_content='=== Early history ==='),\n",
       " Document(metadata={}, page_content='The first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is trained on a text corpus, it can then be used as a probabilistic text generator.'),\n",
       " Document(metadata={}, page_content='Computers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.'),\n",
       " Document(metadata={}, page_content='The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for'),\n",
       " Document(metadata={}, page_content='action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.'),\n",
       " Document(metadata={}, page_content='=== Generative neural networks (2014–2019) ==='),\n",
       " Document(metadata={}, page_content='Since inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.'),\n",
       " Document(metadata={}, page_content='In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.'),\n",
       " Document(metadata={}, page_content='In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.\\nThe new generative models i')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### How to recursively split text into characters\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_docs = text_splitter.create_documents([doc.page_content for doc in docs])\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c1b5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc15e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Web based loader\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[\n",
    "        url,\n",
    "    ],\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\n",
    "                \"post-title\",\n",
    "                \"post-content\",\n",
    "                \"post-header\",\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849a135c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ef188a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1706.03762\").load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a0e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wikipedia\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28772bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Francisco filed a landmark lawsuit to shut down \"undress\" apps that allow users to generate non-consensual AI nude images, citing violations of state laws. The case aligns with California\\'s recent legislation—SB 926, SB 942, and SB 981—championed by Senators Aisha Wahab a')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n",
    "docs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-pg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
